% !TeX spellcheck = en_GB
\chapter{The Chow-Gulliver Critical Planes Result}

The main result we want to establish in this chapter is theorem \ref{chow gulliver}, a result about critical hyperplanes when applying the method of the moving planes to solutions of a large class of non-linear parabolic partial differential equations, and whose proof is somewhat similar to Theorem \ref{Alexandrov theorem}. 

We will first describe the differential equations we are analysing, then prove that they are parabolic, and finally prove the theorem. 

After this, we will include some corollaries of the result and an application to find some estimates for the gradient of the support function and the gradient of the radial function. 

Finally, another application of the result is presented, showing that the condition of \textit{coming out of a point} is particularly rigid on ancient expansive flows. 

\section{Class of Equations we analyze}

We consider manifolds $M^n$ embedded in $\R^{n+1}$, i.e. there is an embedding $X_0 : M^n \rightarrow \R^{n+1}$ parametrizing the hypersurface $X_0(M^n)$. 

Let $F:\{(\kappa_1, \dots , \kappa_n)\in \R^n\vert \kappa_1\leq \dots \leq \kappa_n\}\rightarrow \R$ be a $C^1$ function satisfying:

\begin{equation}
	\frac{\partial F}{\partial \kappa_i} > 0 \mathrm{\; for \; all } \; i=1,\dots, n \label{parabolicità}
\end{equation}
and consider the evolution equation 
\begin{align}
	\begin{dcases}
		\frac{\partial X_t}{\partial t} = - F(\kappa_1(x), \dots , \kappa_n(x)) \nu\\
		X(0)= X_0
	\end{dcases} \label{evolutioneq}
\end{align}
where $\nu$ is the outward normal to $X_t(M^n)$ at the point $X_t(x)$ and $\kappa_1\leq \dots \leq \kappa_n$ are the principal curvatures at $X_t(x)$. 
\begin{oss}\em
	Condition (\ref{parabolicità}) does not need to hold for every possible choice of $\kappa_i$, it just needs to hold at all points of the solution to (\ref{evolutioneq}). 
\end{oss}
\section{Parabolicity of the differential equation (\ref{evolutioneq})}\label{parabolic}

The condition (\ref{parabolicità}) will  guarantee that equation (\ref{evolutioneq}) is a parabolic equation. This may be confusing, as (\ref{evolutioneq}) does not make it obvious how to apply definition \ref{nonlinearpde}. 

In order to justify that this is a parabolic non-linear partial differential equation we can also try to understand how it behaves ``close to a solution" in the solutions space. We want to prove that very close to any solution, ``moving in any direction", the change in the equation is always a parabolic PDE. This will then tell us that our equation is parabolic, and that the theorems that apply to solutions of parabolic partial differential equations apply to our equation as well. To do so, we are going to ``linearise" the differential equation about a solution. 

Following the approach in \cite{huisken}, as $F$ is a symmetric function in the principal curvatures, we may interchangeably take $F$ to be a function of the Weingarten map tensor or of the second fundamental form, and thus we get:

\begin{align*}
	\frac{\partial X_t}{\partial t} &= - F(h_{ij}(X_t)) \nu
\end{align*}

To understand the behaviour close to a solution, we can substitute in our equation $X_t$ with a $X_t+\varepsilon u_t$ to get:

\begin{align}
	\frac{\partial X_t}{\partial t} + \varepsilon\frac{\partial u_t}{\partial t}  &= - F(h_{ij}(X_t+\varepsilon u_t)) \nu_{(X_t+\varepsilon u_t)}\label{linearizingevolutioneq}
\end{align}
where we mean that  $\nu_{(X_t+\varepsilon u_t)}$ is the normal to the perturbed immersion. 
We are interested in the behaviour of this equation for a small $\varepsilon$. This equation when taking the limit for $\varepsilon \rightarrow 0$ is the so-called linearisation of the PDE; we want this PDE to be a parabolic equation to apply our results.

We can use the Weingarten equation (\ref{Weingarten1}) to write the RHS explicitly:

\begin{align*}
	h_{ij}(X_t& + \varepsilon u_t )=-\left\langle v, \nu_{(X_t+\varepsilon u_t)}\right\rangle\; \mathrm{where}\\
	v^\alpha=&\; \frac{\partial^2 X_t^\alpha}{\partial x^i \partial x^j} - \Gamma^k_{ij}\frac{\partial X_t^\alpha}{\partial x^k}+\overline{\Gamma}^\alpha_{\beta \delta}\frac{\partial X_t^\beta}{\partial x^i}\frac{\partial X_t^\delta}{\partial x^k} +\\ %epsilon
	&+\varepsilon\left(\frac{\partial^2 u_t^\alpha}{\partial x^i \partial x^j} - \Gamma^k_{ij}\frac{\partial u_t^\alpha}{\partial x^k}+\overline{\Gamma}^\alpha_{\beta \delta}\left(\frac{\partial X_t^\beta}{\partial x^i}\frac{\partial u_t^\delta}{\partial x^k} + \frac{\partial u_t^\beta}{\partial x^i}\frac{\partial X_t^\delta}{\partial x^k}\right)\right)+\\
	%epsilon squared
	&+\varepsilon^2\left(\overline{\Gamma}^\alpha_{\beta \delta}\frac{\partial u_t^\beta}{\partial x^i}\frac{\partial u_t^\delta}{\partial x^k}\right) \\
	v^\alpha=&\; w +\varepsilon\left(\frac{\partial^2 u_t^\alpha}{\partial x^i \partial x^j} + \mathrm{lower \; order \;  terms}\right) + o(\varepsilon)
\end{align*}
where $h_{ij}(X_t)=-\left\langle w, \nu_{X_t}\right\rangle$.

Putting it all together in the first line:
\begin{align*}
	h_{ij}(X_t + \varepsilon u_t ) &=-\left\langle  w +\varepsilon\left(\frac{\partial^2 u_t}{\partial x^i \partial x^j} + \mathrm{lower \; order \;  terms}\right) + o(\varepsilon), \nu_{(X_t+\varepsilon u_t)}\right\rangle\\
	&= \left\langle  w, \nu_{(X_t+\varepsilon u_t)}\right\rangle -\varepsilon\left\langle  \left(\frac{\partial^2 u_t}{\partial x^i \partial x^j} + \mathrm{lower \; order \;  terms}\right), \nu_{(X_t+\varepsilon u_t)}\right\rangle + o(\varepsilon)\\
	&= h_{ij}(X_t) + \left\langle  w, \nu_{(X_t+\varepsilon u_t)}-\nu_{X_t}\right\rangle -\varepsilon H_{ij} + o(\varepsilon)\\
	&= h_{ij}(X_t) -\varepsilon H_{ij} + o(\varepsilon)
\end{align*}
Were on the last step we are using the fact that $\nu_{(X_t+\varepsilon u_t)}-\nu_{X_t} = O(\varepsilon)$, and as this gets smaller the component of w parallel to the difference also is $O(\varepsilon)$, as $w$ is parallel to $\nu_{X_t}$.
We can then expand F in the RHS of the equation (\ref{linearizingevolutioneq}) to the first order, as it is a $C^1$ function: 
\begin{align*}
	\cancel{\frac{\partial X_t}{\partial t}} + \varepsilon\frac{\partial u_t}{\partial t}  &= - \left(\cancel{F(h_{ij}(X_t)) }
	+ \varepsilon \langle DF, H\rangle + o(\varepsilon)\right)\nu_{(X_t+\varepsilon u_t)}\\
	\varepsilon\frac{\partial u_t}{\partial t}  &= \varepsilon\left(
	\frac{\partial F}{\partial h_{i j}} g^{i k}g^{jl}\left\langle \frac{\partial^2 u_t}{\partial x^k \partial x^l},  \nu_{(X_t+\varepsilon u_t)}\right\rangle + \mathrm{lower \; order \;  terms}\right)\nu_{(X_t+\varepsilon u_t)} + o(\varepsilon)\\
	\frac{\partial u_t}{\partial t}  &= 
	\frac{\partial F}{\partial h_{i j}} g^{i k}g^{jl}\left\langle \frac{\partial^2 u_t}{\partial x^k \partial x^l},  \nu_{(X_t+\varepsilon u_t)}\right\rangle \nu_{(X_t+\varepsilon u_t)} + \mathrm{lower \; order \;  terms} + o(1)
\end{align*}
Letting $\varepsilon\rightarrow 0$, we get to: 

%Here, $\delta h_{ij}(u_t)=\frac{h_{ij}(X_t+\varepsilon u_t)-h_{ij}(X_t)}{\varepsilon}=\frac{h_{ij}(\varepsilon u_t)}{\varepsilon}=h_{ij}(u_t)$ by linearity of the definition of the second fundamental form. 
%\begin{align*}
%	\frac{\partial u_t}{\partial t}  &=  DF(h_{ij}(u_t)) \nu\\
%	\frac{\partial u_t}{\partial t} &= \frac{\partial F}{\partial h_{i j}} g^{i k}g^{jl}h_{k l}(u_t) \nu
%\end{align*}
%
%Using the Weingarten Equations this becomes, implying Einstein's summation convention: 
\begin{align*}
	\frac{\partial u_t}{\partial t} &= \frac{\partial F}{\partial h_{i j}} g^{i k}g^{jl}\left\langle \frac{\partial^2 u_t}{\partial x_k\partial x_l} , \nu \right\rangle \nu+ \mathrm{lower \ order \ terms}
\end{align*}
For the second order term to be positive definite, then, 
\begin{align*}
	\left(\frac{\partial F}{\partial h_{i j}} \right)_{i, j}
\end{align*}
must be positive definite. Or equivalently, as the principal curvatures are the eigenvalues of the matrix $(h_{i j})_{i, j}$,
\begin{align*}
	\frac{\partial F}{\partial \kappa_{i}} > 0
\end{align*}
A more general approach proving that the differential equation is parabolic can be found in \cite{Gerhardt Curvature}. 
\begin{comment}
	{\em Looking more closely at what we are doing, one can prove that it is equivalent to follow this linearisation procedure or to take the derivatives of $F$ with respect to the second order terms.}
\end{comment}

\begin{oss}
	{\em While in this chapter we are using the standard metric of $\R^n$, the calculation above is valid using any other metric. We will take advantage of this fact in the next chapter.}
\end{oss}
\begin{comment}
	contenuto...
	If one considers the F which are linear in the $\kappa_{i}$, this yields:  
	
	\begin{align*}
		\frac{\partial Y_t}{\partial t} &= \left(\frac{\partial F}{\partial\kappa_{i}} \kappa_{i} \right)\nu\\
		\frac{\partial Y_t}{\partial t} &= \frac{\partial F}{\partial\kappa_{i}} o^{ik} h_{kl}o^{li}\,\nu \\ 
		\frac{\partial Y_t}{\partial t} &= \frac{\partial F}{\partial\kappa_{i}}   o^{ik}o^{li}\left\langle \frac{\partial^2 Y_t}{\partial x_k\partial x_l} , \nu \right\rangle \,\nu + \text{lower order terms}
	\end{align*}
	implying summation convention, for an appropriate orthogonal matrix $o_{li}$ that makes $h_{ij}$ diagonal, which makes the equation parabolic if $\frac{\partial F}{\partial \kappa_i}$ is positive. 
	
	{\vspace{10pt}\LARGE \bf [NEED HELP WITH PARABOLICITY!]}
	
	In \cite{huisken}, the paper considers F as a function of the second fundamental form $h_{ij}$, as the principal curvatures are themselves function of it. The following is then said, speaking about "linearisation", while also citing the Weingarten equations:
	\begin{align*}
		\frac{\partial Y_t}{\partial t} &= \frac{\partial F}{\partial h_{i j}} g^{i k}g^{jl}h_{k l} \nu+ \mathrm{lower \ order \ terms}\\
		\frac{\partial Y_t}{\partial t} &= \frac{\partial F}{\partial h_{i j}} g^{i k}g^{jl}\left\langle \frac{\partial^2 Y_t}{\partial x_k\partial x_l} , \nu \right\rangle \nu+ \mathrm{lower \ order \ terms}
	\end{align*}
	Which I did not understand the implication of (especially with respect to maximum principle - there is probably something I am missing due to non-linearity of the equation). 
	
	\begin{theorem}[Parabolicity of the differential equation (\ref{evolutioneq})]
		da scrivere\label{graphparabolic}
	\end{theorem}
	
	Il conto probabilmente è quello in \cite{hamilton}, pagina 262. Info sul nostro caso sono in \cite{huisken}, pagina 50. 
	
	{\vspace{10pt}\LARGE \bf [NEED HELP!]}
	
\end{comment}

\section{An existence result}
In \cite{huisken} one finds the following comforting existence result for the class of equations we are analysing under very broad hypothesis. The same paper also includes a proof of the result with some more restrictive hypotheses. A complete proof is quite more involved. A much more extended analysis of the problem of the existence of solution to the equation can be found in \cite{Gerhardt Curvature}. 


\begin{theorem}[Short term existence of a solution for (\ref{evolutioneq})]\label{shorttermexistance}
	Suppose $X_0 : M^n \rightarrow \R^{n+1}$ is a smooth, closed hypersurface in $\R^{n+1}$, such that (\ref{parabolicità}) holds at all points in $X_0$, i.e. for all the values of the principal curvatures $\kappa_{i}$ realized at some point on $X_0$. Then, (\ref{evolutioneq}) has a smooth solution, at least on some short time interval $[0, T)$, $T > 0$.  \label{existence}
\end{theorem}

\section{Local representation as a graph of a solution of (\ref{evolutioneq})}
\label{representation as graph}
As a first step, from the Corollary \ref{localgraphcorollary}, we can establish the following:

\begin{theorem}[Local representation as a graph of a solution of (\ref{evolutioneq})]
	Let $X^n$ be a submanifold $X^n\subset\R^{n+1}$ and let $F:X^n\times(0, T)\rightarrow \R^{n+1}$ be a solution of (\ref{evolutioneq}). Also let $t\in(0, T)$ and $x\in F(X^n, t)$.
	Then there exists a neighbourhood of $(x, t)$, $U\subset F(X^n\times(0, T))$, and a smooth function $f: T_{x} F(X^n, t)\times(0, T)\rightarrow \R$ such that any $(x_0, t_0)\in U$ can be expressed as 
	\begin{align*}
		x_0= p +f(p, t_0) \nu 
	\end{align*}
	where $\nu$ is a vector normal to $T_{x} F(X^n, t)$, for an appropriate point $p\in T_x F(X^n, t)$. \label{localgraph}
\end{theorem}
\begin{comment}
	{\vspace{10pt}\LARGE \bf [NON FUNZIONA! Difficile (impossibile?)dimostrare che è smooth]}
	With a reasoning similar to \ref{localgraphclassic} and \ref{localgraphcorollary} one can prove that given any plane, we can represent locally a manifold $M\subset\R^{n+1}$ at $x\in M$ as a graph on that plane, i.e. as $x_0= p + f(p) \nu$ for $\nu$ the vector orthogonal to the plane, as long as $\nu\notin T_xM$. 
	The function which associates to $t\in [0,T)$ the unit vector orthogonal to $T_{F(x, t)}F(X, t)$ is continuous, as $F$ is smooth and the vector can be computed from the derivatives of the local maps of the manifold. Thus we can find a neighbourhood $(a,b)$ of $ t_0$ such that for $t \in (a,b)$ the normal vector is not in $T_x F(X^n, t)$. In $(a,b)$ we can represent $F(X)
\end{comment}


\begin{proof}
	We can consider the image of $F:X^n\times(0, T)\rightarrow \R^{n+1}$ as a manifold in $\R^{n+2}$ by considering $G:=(F(x, t), t)$. Moreover $\frac{\partial G_t}{\partial e_j}\equiv 0$ for all possible vectors of the canonical basis of $\R^{n+1}\times \R$ except for the one corresponding to the time coordinate, where it is $1$. Also, $\frac{\partial G_i}{\partial e_j}\equiv \frac{\partial F_i}{\partial e_j}$ and the first $n$ coordinates of  $\frac{\partial G}{\partial t}$ form a vector normal to $T_x X^n$ by (\ref{evolutioneq}).  Thus, the tangent space of $\mathrm{Im}(G)$ is $T_x X^n \times \{0\}\oplus  \mathrm{span}\langle (\nu, 1)\rangle$ and we can apply corollary  \ref{localgraphcorollary} to $\mathrm{Im}(G)$ to get a function $\tilde{f}$ such that 
	\begin{align*}
		(x_0, t)= \left[(p, 0) +  (s\nu, s)\right] + \tilde{f}(p, s)(\nu, -\Vert \nu\Vert)
	\end{align*}
	as $(\nu, -\Vert \nu\Vert)$ is the vector orthogonal to  $T_x X^n \times \{0\}\oplus  \mathrm{span}\langle (\nu, 1)\rangle$. Let $\sigma_p:I\subset\R\rightarrow\R$ be the function that associates to $t$ the appropriate $s$ in the expression above.  Projecting to the first $n+1$ coordinates and calling $f(p, t)=\sigma_p(t)+\tilde{f}(p, \sigma_p(t))$ one gets:
	\begin{align*}
		x_0= p + f(p, t)\nu 
	\end{align*}
	which is our thesis, as long as $\sigma_p(t)$ is smooth. This is indeed the case, as the graph function $\Gamma_f:x\mapsto(x, f(x))$ is smooth for any smooth function $f$ and has a smooth inverse (and thus, the inverse $(x_0, t)\mapsto ((p, 0) +  (s\nu, s))$ is smooth).
\end{proof}
\begin{oss}\em
	One can show through direct calculation (see \cite{mantegazza}, Exercise 1.1.2) that, if an immersed hypersurface $\phi : M\rightarrow \R^{n+1}$ is locally the graph of a function $f:\R^n \rightarrow \R$ (i.e., locally, $(x, f(x))=\phi$), then: 
	\begin{align*}
		g_{ij}&=\delta_{ij}+ \frac{\partial f}{\partial x_i} \frac{\partial f}{\partial x_j}\\
		\nu&= -\frac{(\nabla f, -1)}{\sqrt{1+|\nabla f|^2}}\\
		h_{ij}&= -\frac{H_{ij}}{\sqrt{1+|\nabla f|^2}}
	\end{align*}
	Where the matrix $(H_{ij})_{ij}$ is the hessian of $f$. Thus, if one considers the principal curvatures, they are closely related to the eigenvalues of the hessian of $f$. This could also be used to do the calculation in section 2.2 but it may be harder to extend it to constant curvature spaces. 
\end{oss}



\section{The Moving Planes Method and the Chow result}

We will now present the Chow-Gulliver result, roughly following paragraph 2 of the original paper \cite{Chow}.
Suppose that we have a hypersurface embedded in $\R^{n+1}$ evolving according to equation (\ref{evolutioneq}). For a fixed time $t$, we can apply the method of the moving planes as described in section \ref{method moving planes}: we can take parallel hyperplanes $\pi_{v,s}$ orthogonal to $v$ intersecting $X$, and consider the reflection  $X_{v,s}^\pi$. There will be a hyperplane $\pi_{v,m_v}$ where $X$ and $X_{v,s}^\pi$ are tangent. As the hypersurface evolves, we may wonder how this critical threshold changes over time. If it behaves in a predictable way, we can hope to use the Moving Planes Method on the evolving manifold, otherwise, if the critical plane moves back and forth multiple times, it may be a hopeless endeavour. In the next section, we are going to prove a result in this general direction, to show a form of "regularity" in this sense. First, however, we need to introduce a marginally stricter definition for the concept of "reflecting inside itself". 



Let $\pi$ be a hyperplane in $\R^{n+1}$. We may assume $\pi$ orthogonal to a unit vector $v\in\R^{n+1}$, i.e. $\langle x, v\rangle= C$ for all $x\in \pi$ for some constant $C$. In our notation for the method of moving planes, assuming we pick the origin as a starting point, this means that $\pi = \pi_{v, C}$. 

Then, $\R^{n+1}$ is divided by $\pi$ into two half-spaces, which we will name 
\begin{align*}
H^+(\pi)&=\{x \in \R^{n+1} : \langle x, v\rangle > C\}= \bigcup_{s>C} \pi_{v, s} \;\;\mathrm{and}\\
H^-(\pi)&=\{x \in \R^{n+1} : \langle x, v\rangle < C\}= \bigcup_{s<C} \pi_{v, s}.
\end{align*} 

\begin{defin}
	We say {\em we can reflect $X: M^n\rightarrow \R^{n+1}$ strictly with respect to $\pi$} if both:
	\begin{itemize}
		\item $X^\pi\cap H^-(\pi)\subset \mathrm{int}(X)\cap H^-(\pi)$ where $X^\pi$ is the reflection of $X$ about $\pi$ and $\mathrm{int}(X)$ is the region inside $X$.
		\item $V\notin T_xM$ for all $x\in M^n \cap\pi$
	\end{itemize} \label{strict-reflection-definition}
\end{defin}


\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{"figures/7_reflects_strictly"}
	\caption{Example: We can reflect $X: M^n\rightarrow \R^{n+1}$ strictly with respect to $\pi$}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{"figures/8_interior_contact"}
	\caption{Example: We cannot reflect $X: M^n\rightarrow \R^{n+1}$ strictly with respect to $\pi$, because there is an interior contact}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=\textwidth]{"figures/9_boundary_contact"}
\caption{Example: We cannot reflect $X: M^n\rightarrow \R^{n+1}$ strictly with respect to $\pi$, because the tangent spaces coincide at a point about which we are reflecting}
\end{figure}

This fundamentally means that the reflection of one of the halves of $X$ on the other side of $\pi$ is contained in the region inside $M^n$ and the tangent spaces of $X$ and of the half-reflection do not form a ninety degree angle with $\pi$, at all points on $\pi\cap X$. As the two tangent spaces are one the reflection of the other, this means that they do not coincide.   
\begin{defin}
	We say {\em we can reflect $X: M^n\rightarrow \R^{n+1}$ strictly up to $(\pi,v)$} if we can reflect $M^n$ strictly with respect to $\pi_{v, s}$ for all hyperplanes $\pi_{v, s}$ such that $s<C$.  
\end{defin}

The key idea of the main result in the next section is as follows: suppose we have an embedded smooth hypersurface $X$ evolving according to (\ref{evolutioneq}) and a fixed hyperplane $\pi$, intersecting $X$. Suppose that, at some time $t$, $X$ and  $X_\pi$ touch outside of $\pi$. We can consider $X$ and  $X_\pi$ as local graphs over the same hyperplane $\pi$, and we can show that these function evolve according to the same differential equation. Using the strong maximum principle and the Hopf boundary point lemma, then, one can conclude that the two functions coincide, and have been coinciding up until that point. We can then conclude that if  $X$ and  $X_\pi$ only touch in $X\cap\pi$ at the beginning of the evolution, then they will never touch elsewhere.  


\section{The Chow-Gulliver result}\label{main theorem section}

The main theorem is the following: 

\begin{theorem}[Chow-Gulliver]\label{chow gulliver}
	Let $X:M^n\times [0,T) \rightarrow \R^{n+1}$ be a $C^2$ solution to equation (\ref{evolutioneq}). Then, if we can reflect $X(M^n, 0)=X_0$ strictly with respect to $\pi$, then for all $t\in [0,T)$ we can reflect $X(M^n, t)=X_t$ strictly with respect to $\pi$. 
\end{theorem}

\begin{proof}
	By contradiction, suppose that there is a time $t$ such that the thesis is false, and that it is the smallest such $t$. Then, for all $\tau \in [0,t)$, $X_{\tau,\pi}\cap H^-(\pi)\subset \mathrm{int}(X_{\tau})\cap H^-(\pi)$; the unit vector orthogonal to $\pi$, $V$, is such that $V\notin T_xX_\tau$ for all $x\in X_\tau\cap \pi$ and $\tau \in [0,t)$; and either of the conditions fails at $t$, i.e. either: 
	\begin{itemize}
		\item[(i)] $X_{t,\pi}\cap H^-(\pi)\cap X_{t}\neq \emptyset$
		\item[(ii)] $V\in T_xX_t$  for some $x\in\pi$. 
	\end{itemize} 
	
	(i) Suppose the first case is true. Then, there exists $x_0 \in X_{t,\pi}\cap H^-(\pi)\cap X_{t}$ such that at $x_0$ the two manifolds are tangent. \\
	We can take a neighbourhood of $(x_0, t)\in X_{t}\times \R$ such that  both $X_{t,\pi}$ and $X_{t}$ are graphs over $T_{x_0}X_{t}$ by \ref{localgraph}. \\
	We can explicitly write the functions $f:U\times (t-\varepsilon, t+\varepsilon)\rightarrow X_t$, where $U\subset T_{x_0}X_{t}$, and the corresponding $f_\pi$ for $X_{t,\pi}$. 
	We can also write 
	\begin{align*}
		f \; : \; (x, t) &\mapsto x+\tilde{f}(x, t)\nu \\
		f_\pi \; : \; (x, t) &\mapsto x+\tilde{f}_\pi(x, t)\nu 
	\end{align*}
	for appropriate functions $\tilde{f}:U\times (t-\varepsilon, t+\varepsilon)\rightarrow \R$ and $\tilde{f}_\pi:U\times (t-\varepsilon, t+\varepsilon)\rightarrow \R$, where $\nu$ is a fixed unit vector normal to $T_{x_0}X_{t}$.  $\tilde{f}$ and $\tilde{f_\pi}$ are solutions to the same second order PDE, which is parabolic by what was discussed in paragraph \ref{parabolic}, hence we can apply Proposition \ref{firstapplication} to conclude that $\tilde{f}\equiv\tilde{f_\pi}$, and thus $X_{t,\pi}$ and $X_{t}$ coincide in a neighbourhood of $(x, t)$, a contradiction as we assumed that $t$ is the first $t$ where the flows touch.
	
	(ii) Suppose instead that $V\in T_xX_t$  for some $t\in [0, t)$ and some $x\in X_t\cap \pi$. Then $T_xX_t= T_xX_{t, \pi}$ and in a neighbourhood of $(x, t)$ both $X_t$ and $X_{t, \pi}$ are graphs of two smooth functions over $T_xX_t$ by \ref{localgraph}, i.e. again
	\begin{align*}
		f \; : \; (x, t) &\mapsto x+\tilde{f}(x, t)\nu \\
		f_\pi \; : \; (x, t) &\mapsto x+\tilde{f}_\pi(x, t)\nu 
	\end{align*} 
	Moreover, in $\overline{H^-(\pi)}$, $f_\pi\geq f$, because $M^n_\pi\cap H^-(\pi)\subset \mathrm{int}(M^n)\cap H^-(\pi)$. Finally, $f(x, t)=f_\pi (x, t)$, hence $f_\pi-f (x, t)=0$, and thus  $(x, t)$ is a minimum point on the boundary for $f_\pi-f$. Also, we must have
	\begin{align*}
		\frac{\partial f}{\partial V}(x,t)=\frac{\partial f_\pi}{\partial V}(x,t)
	\end{align*}
	because the graphs are both tangent to $T_xX_t$, and $V$ here is the outward pointing normal to the boundary by definition of the reflection. Thus, 
	\begin{align*}
		\frac{\partial (f- f_\pi)}{\partial V}(x,t)=0
	\end{align*}
	But we must have 
	\begin{align*}
		\frac{\partial (f- f_\pi)}{\partial V}(x,t)>0
	\end{align*}
	at a minimum on the boundary by Proposition \ref{secondapplication}, a contradiction.  
\end{proof}


\section{Some corollaries of the result} \label{Some corollaries of the result}
In this section we collect a number of corollaries to the main result above. This first one is an immediate consequence of the main result:
\begin{cor}
	Let $X:M^n\times [0,T) \rightarrow \R^{n+1}$ be a $C^2$ embedded solution to equation (\ref{evolutioneq}). Then, if we can reflect $X_0$ strictly up to $(\pi_{v,C},v)$, for all $t\in [0,T)$ $v\notin T_xX_t$ for all $x\in X_t\cap\overline{H^+(\pi)}$. In particular,  $ X_t\cap\overline{H^+(\pi)}$ is a graph over $\pi$ for all $t\in [0,T)$.\label{graph}
\end{cor}




Another consequence of theorem \ref{chow gulliver} is the following:
\begin{cor}
	Let $X:M^n\times [0,T) \rightarrow \R^{n+1}$ be a $C^2$ solution to equation (\ref{evolutioneq}). Then, if we can reflect $X_0$ strictly up to $(\pi_{v,C},v)$, for all $t\in [0,T)$ we can reflect $X(M, t)$ strictly up to $(\pi_{v,C},v)$. \label{upto}
\end{cor}
\begin{proof}
	The hypothesis of the theorem are true for each $\pi_K$ in the definition, thus we can reflect strictly with respect to each $\pi_K$ for all $t\in[0,T)$, and thus we can reflect $X(M, t)$ strictly up to $(\pi_{v,C},v)$.
\end{proof}

 Furthermore, it is clear that, for every direction $v$, there exists a hyperplane $\Pi$, perpendicular to $v$, such that we can reflect $X_0$ up to $(\Pi, v)$ and $\Pi$ intersects the interior of $X_0$. To be more precise, for every direction $v$, there exists a hyperplane $\Pi_0^v$ tangent to $X_0$ and such that $X_0\cap H^+(\Pi_0^v)=\emptyset$. Suppose that for every $\varepsilon>0$ we can find a plane $\pi_\varepsilon$ such $H^+(\pi_\varepsilon)\cap B_{R-\varepsilon}(C)=\emptyset$ and we cannot reflect $X_0$ strictly at $\pi_\varepsilon$, where $R$ is such that $X_0 \subset B_R (C)$. We can take the corresponding plane $\Pi_0^\varepsilon$ parallel to $\pi_\varepsilon$ and tangent to $X_0$ at the point $p_\varepsilon$. Taking a sequence $\varepsilon_n \rightarrow 0$ we find a corresponding limited sequence $p_{\varepsilon_n}$ which, by compactness, has a subsequence converging to a point $p\in X_0$. By construction, this point $p$ is such that arbitrarily close to it there is a point in $X_0$ such that we cannot reflect by more than any chosen $\varepsilon>0$ in the direction of its normal. As we can always represent $X_0$ as the graph of a function in a neighbourhood of $p$, we get a contradiction, because strict reflection in the direction of the normal by at least a fixed uniform amount $\varepsilon$ at each point is always possible locally for any graph of a smooth function. Thus, we obtain the following:
 
\begin{cor}
	Let $X:M^n\times [0,T) \rightarrow \R^{n+1}$ be a $C^2$ embedded solution to equation (\ref{evolutioneq}). There exists $\varepsilon>0$ depending only on $X_0$ such that for all $t\in[0, T)$ we can reflect $X_t$ up to $(\Pi_0^v +\epsilon v, v)$ for every $v \in S^n$. In particular, if $X_0 \subset B_R (C)$, then we can always reflect $X_t$ up to $(\Pi, v)$ whenever $H^+(\Pi)\cap B_{R-\varepsilon}(C)=\emptyset$.\label{reflect a small bit}
\end{cor}
In other words, we can always reflect a little $\varepsilon$ in any direction, uniformly, from $X_0$. We can use the fact that we can always reflect about a plane outside a sphere containing $X_0$ to prove the following estimate:




\begin{cor}
	Let $X:M^n\times [0,T) \rightarrow \R^{n+1}$ be a $C^2$ embedded solution to equation (\ref{evolutioneq}). There exists $C>0$ depending only on $X_0$ such that for all $t\in[0, T)$: 
	\begin{align*}
		\max_{x\in X_t} |x| - \min_{x\in X_t} |x| < C
	\end{align*}\label{sandwich estimate}
\end{cor}
\begin{proof}
	We can reflect $X_0\subset B_R(0)$ up to any plane tangent to  $B_R(0)$, i.e. any plane $\pi_{v, K}$ for any $K\geq R$ and $v$ unit vector, i.e.  $\pi_{v, K}=\{p \in \R^{n+1} : \langle p, v \rangle = K\}$. Let $x_1, x_2\in X_t$ such that $|x_1|=\min_{x\in X_t} |x|$ and  $|x_2|=\max_{x\in X_t} |x|$. Let $v=\frac{x_2-x_1}{\vert x_2-x_1 \vert}$: we can reflect $X_t$ up to $(\pi_{v, R}, v)$ by theorem \ref{chow gulliver}, therefore $\mathrm{dist}(x_2, \pi_{v, R}) < \mathrm{dist}(x_1, \pi_{v, R})$, or in other words:
	\begin{align*}
		 \left\langle x_2, \frac{x_2-x_1}{\vert x_2-x_1 \vert} \right\rangle - R &< 
		 \left\langle x_1, \frac{x_1-x_2}{\vert x_2-x_1 \vert} \right\rangle + R\\
		 \frac{|x_2|^2}{\vert x_2-x_1 \vert} - \cancel{\frac{\left\langle x_2, x_1\right\rangle}{\vert x_2-x_1 \vert}}  - R &< 
		 \frac{\vert x_1\vert^2}{\vert x_2-x_1 \vert} - \cancel{\frac{\left\langle x_2, x_1\right\rangle}{\vert x_2-x_1 \vert}}  + R\\
		 \frac{|x_2|^2 - |x_1|^2}{\vert x_2-x_1 \vert} &< 
		  2R\\
		  |x_2|^2 - |x_1|^2 &< 
		  2R \vert x_2-x_1 \vert \leq 4R |x_2|\\
		  |x_2|^2 &<|x_1|^2 + 4R |x_2|\\
		  |x_2| &< |x_1| \frac{|x_1|}{|x_2|} + 4R < |x_1| + 4R\\
		  |x_2| -|x_1| &<4R
	\end{align*}
\end{proof}

\begin{oss}
	\em This result has an important meaning if the hypersurface uniformly expands to infinity, i.e. $\lim_{t\rightarrow T}\min_{x\in X_t} |x| =\infty$. We can then consider the rescaled hypersurfaces 
	\begin{align*}
		\widetilde{X_t} = \frac{1}{\min_{x\in X_t} |x|} X_t
	\end{align*}
	Immediately, we find that  the $\widetilde{X_t}$ must converge uniformly to a sphere, because $\frac{C}{\min_{x\in X_t} |x|}\rightarrow 0$, and therefore 
	\begin{align*}
		\max_{x\in \widetilde{X_t}} |x| - \min_{x\in \widetilde{X_t}}|x|\rightarrow 0
	\end{align*}
\end{oss}

The following result about the part of $X_t$ outside a ball is also a surprisingly powerful tool: 



\begin{cor}
	Let $ X : M^n \times [0, T) \to \mathbb{R}^{n+1} $ be an embedded solution to equation (\ref{evolutioneq}). Then, if, for a sphere $B$, $X_0\subset B$, at all times $t \in [0, T)$ $X_t\setminus B$ is star-shaped with respect to the centre of $B$.\label{starshaped}
\end{cor}

\begin{proof}
	$X_0\subset B$ therefore we can reflect $X_t$ about any hyperplane tangent to $B$ by Corollary \ref{reflect a small bit}. By Corollary \ref{graph} $X_t\cap\overline{H^+(\pi)}$ is a graph over $\pi$, therefore, it is not possible for a normal line coming out of $B$ and orthogonal to $\pi$ to intersect  $X_t\cap\overline{H^+(\pi)}$ more than once. This implies that  $X_t\setminus B$ is star-shaped with respect to the centre of $B$
\end{proof}

Lastly: 
\begin{cor}
	Let $X:M^n\times [0,T) \rightarrow \R^{n+1}$ be a $C^2$ embedded solution to  equation (\ref{evolutioneq}). Let $s_v:[0,T) \rightarrow I$ be such that 
	\begin{align*}
		s_v(t)= \sup\left\{s \in I \;|\;  \; \mathrm{we \; can \; reflect \;} X_t \mathrm{ \; strictly \; up \; to }\; (\pi_{v, s}, v) \right\}.
	\end{align*}
	Then $s_v(t)$ is a non-decreasing function. Also, if $X_0$ is compact, the limit
	\begin{align*}
		\lim_{t\rightarrow T^-} s_v(t)
	\end{align*}
	exists and is finite.
\end{cor}
\begin{proof}
	When taking $X_t$ as the starting manifold, the hypothesis of the theorem are still true, therefore we can reflect about $\pi_{c, v}$  for all $c<s_v(t)$ at all subsequent times. Thus $s_v(t)$ is non-decreasing. $\lim_{t\rightarrow T^-} s_v(t) = \sup s_v(t)$, therefore the limit exists. Also, if $X_0$ is bounded, there exists $R>0$ such that $X_0\subset B_R(0)$, therefore we can reflect $X_0$ strictly about any hyperplane non intersecting $B_R(0)$, as it does not touch $X_0$, and therefore we can also reflect $X_t$ strictly about the same hyperplanes by theorem \ref{chow gulliver}. At the same time, there exists a hyperplane such that $X_t$ cannot be reflected strictly about it, because there will always be a straight line parallel to $v$ intersecting $X_t$ at more than one point, letting us consider the hyperplane orthogonal to $v$ passing through their midpoint, about which $X_t$ cannot be reflected strictly, implying $s_v(t) \neq +\infty$. Therefore  $s_v(t)\in [-R, R]$, and the limit above is finite. 
\end{proof}





\section{Applying the result to find gradient estimates}\label{Applying the result to find gradient estimates}

In this section we collect some applications of theorem \ref{chow gulliver} to gradient estimates for the support function and the radial function, originally included in \cite{Chow} (a more in-depth definition these two functions can be found in the introduction of \cite{tomography}). Central in what will follow is this corollary providing an estimate for the tangent component of the position vector $x$ of a point on the hypersurface in its own tangent space: 
\begin{cor}
	Let $ X : M^n \times [0, T) \to \mathbb{R}^{n+1} $ be an embedded solution to equation (\ref{evolutioneq}). There exists a constant $ C $, depending only on the initial hypersurface $ X_0 $, such that for all points $ x \in X_t $ and $ t \in [0, T) $, the following inequality holds:
	\begin{align*}
		| x - \langle x \cdot \nu\rangle \nu | \leq C,
	\end{align*}
	where $ \nu $ is the unit normal to $ X_t $ at the point $ x $.
	\label{x projection estimate}
\end{cor}


\begin{proof}: 
	Choose $ C > 0 $ such that $ X_0 \subset B_C(0) $. By Theorem \ref{chow gulliver}, we can reflect $ X_t $ strictly up to any plane tangent to the ball, like in the previous corollaries.	
	Thus, for any point $ x \in X_t $ and outside the ball, we know that whenever $ (x, V) > C $, then $ V \notin T_x X_t $.
	This is equivalent to saying that for all $ W \in T_x X_t $, we have:
	\begin{align*}
		(x, W) \leq C.
	\end{align*}
	If we take now the projection of $x$ on $T_x X_t$ and rescale it to be a unit vector, $ W = \frac{x - (x \cdot \nu) \nu}{|x - (x \cdot \nu) \nu|} \in T_x X_t $, then we obtain:
	\begin{align*}
		C \geq (x, W) = \frac{|x - (x \cdot \nu) \nu|}{|W|} = |x - (x \cdot \nu) \nu|.
	\end{align*}	
	Thus, the corollary is proved.
\end{proof}


\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{"figures/10_geometric_idea"}
	\caption{A geometric interpretation of corollary \ref{x projection estimate}: the normal line always intersects $B_C(0)$. $x^\top = x - (x \cdot \nu) \nu$.}
\end{figure}

Let's now assume that the hypersurfaces $X_t$ in the solution to the equation are convex. We are going to show a gradient estimate for the support function of the hypersurface. Support functions are one of the most important concepts stemming from the study of convex sets. 
\begin{defin}
	Let $K$ be a non-empty compact convex set in $\R^n$. We define the {\em support function} $u_K:  S^n \to \mathbb{R} $ as 
	\begin{align*}
		u_K (\nu) = \sup\{\langle x, \nu \rangle : x\in K \}
	\end{align*}
\end{defin}

\begin{oss}
	\em Many authors define the support function on the whole euclidean space. As its value (thus $\nabla u_K$) scales linearly with the distance from the origin, we will not be doing so out of simplicity. Notice that given two convex sets $K_1$ and $K_2$, $K_1 \subseteq K_2$ if and only if   $u_{K_1} \leq u_{K_2}$. In this sense, a convex set is determined by its support function. Also, the tangent plane at the point in $\partial K$ which has $\nu$ as a normal vector is  $h_\nu=\{x\in \R^n : x \cdot \nu = u_K (\nu)\}$. 
\end{oss}

\begin{cor}
	Let $ u : S^n \times [0, T) \to \mathbb{R} $ be the support function of convex hypersurfaces $ X_t $, solving the equation  (\ref{evolutioneq}). There exists a constant $ C $, depending only on $ u(0) $, such that:
	\begin{align*}
		|\nabla u(\nu, t)| \leq C,
	\end{align*}
	for all $ (\nu, t) \in S^n \times [0, T)$.
\end{cor}


\begin{proof} 
	For each unit normal vector $ \nu \in S^n $, let $ x_t \in X_t $ be the unique point such that $ \nu $ is the outward unit normal to $ X_t $ at $ x_t $. We compute $\nabla u(\nu)$: let $R_\theta$ be a rotation of an angle $\theta$ in the direction $\partial_i\in T_\nu S^n$:	
	\begin{align*}
		\partial_i u(\nu)  &= \lim_{\theta \rightarrow 0} \left[\langle x_t(R_\theta \nu) , R_\theta \nu\rangle - \langle x_t(\nu) ,  \nu\rangle\right]/\theta\\
		 &= \lim_{\theta \rightarrow 0} \left[\langle x_t(R_\theta \nu) , \nu - \nu + R_\theta \nu\rangle - \langle x_t(\nu) ,  \nu\rangle\right]/\theta\\
		 &= \lim_{\theta \rightarrow 0} \left[\langle x_t(R_\theta \nu) ,  R_\theta \nu - \nu\rangle + \langle  x_t(R_\theta \nu)  - x_t(\nu) ,  \nu\rangle\right]/\theta\\ 
		 &= \lim_{\theta \rightarrow 0}\left[ \left\langle x_t(R_\theta \nu) , \frac{ R_\theta \nu - \nu}{\theta}\right\rangle + \left\langle  \frac{x_t(R_\theta \nu)  - x_t(\nu)}{\theta} ,  \nu\right\rangle\right]\\
		 &= \left\langle x_t(\nu) , \partial_i\right\rangle + \cancel{\left\langle  (dx_t)_{\nu}(\partial_i),  \nu\right\rangle}
	\end{align*}
	where $\left\langle  (dx_t)_{\nu}(\partial_i),  \nu\right\rangle=0$ because $ (dx_t)_{\nu}(\partial_i) \in T_{x_t}X_t$. Therefore
	\begin{align*}
		\nabla u(\nu) &= (x_t)^\top =x_t - (x_t)^\bot \\
		\nabla u(\nu) &= x_t - \langle x_t , \nu\rangle \nu= x_t - u(\nu) \nu\\
		|\nabla u(\nu, t)| &= | x_t - \langle x_t , \nu\rangle \nu |\leq C 
	\end{align*}
	by applying Corollary \ref{x projection estimate},  which completes the proof.
\end{proof}


Now, let’s consider the case where the hypersurfaces $ X_t $ are starshaped for all $ t \in [0, T) $. We obtain a gradient estimate for the radial function at points outside a certain compact starshaped region associated with the initial hypersurface $ X_0 $.
\begin{defin}
	Suppose that $ X : M^n \times [0, T) \to \mathbb{R}^{n+1} $ parametrizes starshaped hypersurfaces $ X_t $ with respect to the origin. The {\em radial function} $ r : S^n \times [0, T) \to \mathbb{R}^+ $ is defined so that for each $ (z, t) \in S^n \times [0, T) $, the point $ r(z, t) z $ belongs to $ X_t $. 
\end{defin}

We will need the following lemma: 

\begin{lemma}
	In the hypothesis of the definition above, there exists a constant $ C $, depending only on $ X_0 $, such that for all points $ (v, t) \in S^n \times [0, T) $, we have:
	\begin{align*}
		r^2 |\nabla r|^2 \leq C (r^2 + |\nabla r|^2).
	\end{align*}
	In particular, if $ r^2 > C $, then:
	\begin{align*}
		|\nabla r|^2 \leq \frac{C r^2}{r^2 - C}.
	\end{align*}
\end{lemma}


\begin{proof}
	As $ x = r(z) z $, let $\partial_i\in T_zS^n$ and $\overline{\partial_i}=\partial_i x \in T_zX_t$ for $i=1 \dots n$ be corresponding bases in the tangent spaces. Computing this explicitly in $\R^{n+1}$ yields:
	\begin{align*}
		\overline{\partial_i} &= \partial_i x\\
		&=\partial_i (r(z) z)\\ 
		&= (\partial_i r(z))z + r(z)\partial_i 
	\end{align*}
	Notice here that $z$ and the $\partial_i$ are orthogonal, therefore $|az+b^i\partial_i|^2 = a^2+\sum_i (b^i)^2$. 
	Computing this vector's scalar product with $r(z) z - \nabla r(z)$ yields: 
	\begin{align*}
		\langle  r(z) z - \nabla r(z), \overline{\partial_i} \rangle&= 
		\langle  r(z) z - \sum_j\partial_j r(z) \partial_j, \overline{\partial_i} \rangle\\&= 
		\langle  r(z) z, \overline{\partial_i} \rangle -
		\langle  \sum_j\partial_j r(z) \partial_j, \overline{\partial_i} \rangle\\
		 &=  r(z) \langle z , \overline{\partial_i} \rangle - \sum_j \partial_j r(z) \langle  \partial_j, \overline{\partial_i} \rangle\\
		&=   r(z)(\partial_i r(z)) - \partial_i r(z)r(z) 
		\\&= 0
	\end{align*}
	Moreover, notice that $\nabla r(z)\in T_zX_t$, which is orthogonal to $z$. 
	Therefore, the following formula holds for the normal at a point with respect to the radial function: 
	\begin{align*}
		\nu = \frac{ r(z) z - \nabla r(z) }{\sqrt{r(z)^2  + |\nabla r(z)|^2}}
	\end{align*}
	when taking $ x = r(z) z $. Substituting the expressions above into the inequality from Corollary \ref{x projection estimate}, we obtain:
	\begin{align*}
		C&\geq\left\vert r(z) z - \langle r(z) z ,  r(z) z - \nabla r(z) \rangle \frac{ r(z) z - \nabla r(z) }{r(z)^2  + |\nabla r(z)|^2} \right\vert \\
		&\geq\left\vert r(z) z -  \frac{ r(z)^2(r(z) z - \nabla r(z)) }{r(z)^2  + |\nabla r(z)|^2} \right\vert \\
		&\geq\left\vert\frac{ r(z) z (r(z)^2  + |\nabla r(z)|^2)  -r(z)^2(r(z) z - \nabla r(z)) }{r(z)^2  + |\nabla r(z)|^2} \right\vert \\
		&\geq\left\vert\frac{ r(z)|\nabla r(z)|^2z    + r(z)^2 \nabla r(z) }{r(z)^2  + |\nabla r(z)|^2} \right\vert
	\end{align*}
	Multiplying by $r(z)^2  + |\nabla r(z)|^2$ and squaring (still using the fact that $z$ and $\nabla r(z)$ are orthogonal):
	\begin{align*}
		\left\vert r(z)|\nabla r(z)|^2z  + r(z)^2 \nabla r(z) \right\vert &\leq C (r(z)^2  + |\nabla r(z)|^2)\\
		r(z)^2|\nabla r(z)|^4  + r(z)^4 |\nabla r(z)|^2  &\leq C^2 (r(z)^2  + |\nabla r(z)|^2)^2\\
		r^2 |\nabla r|^4 + r^4 |\nabla r|^2 &\leq C^2 (r^2 + |\nabla r|^2)^2.
	\end{align*}
	From here, the lemma follows.
\end{proof}

The estimate is:

\begin{proposition}
	With the same hypotheses as the lemma above, there exists a constant $ C $, depending only on $ X_0 $, such that for all $ (v, t) \in S^n \times [0, T) $, taking $\Pi_0^v$ as in corollary \ref{reflect a small bit}, if $ r(v, t) v \in \overline{H^+(\Pi_0^v)} $, then:
	\begin{align*}
		| \nabla r(v, t) | \leq K.
	\end{align*}
\end{proposition}

\begin{proof}
		By Corollary \ref{reflect a small bit}, there exists a constant $ \epsilon > 0 $, depending only on $ X_0 $, such that for all $ (z, t) \in S^n \times [0, T) $ with $ r(z, t) z \in H^+(\Pi_0^v) $, and for all $ w \in S^n $ with $ \langle W, z \rangle > 1 - \epsilon $, we can reflect $ X_t $ up to the hyperplane $ \Pi_w = \{r(z, t) z + p : \langle p , w\rangle=0 \}$. This means that $ w \notin T_{r(z,t) z} X_t $, i.e., $ W $ is not tangent to $ X_t $. In the proof of the previous lemma we showed that $r(z) z - \nabla r(z)$ is normal to the manifold's tangent plane at $r(z) z$, so, letting $w_z$ be the projection of $w$ in the $z$ direction: 
	\begin{align*}
		0&< \langle w, r(z) z - \nabla r(z)  \rangle\\
		\langle w,  \nabla r(z)  \rangle &< \langle w, r(z) z   \rangle\\
		\langle w - w_z,  \nabla r(z)  \rangle &< r(z) \langle w,  z   \rangle\\
		|w - w_z||\nabla r(z)|  &<  r(z) \langle w,  z   \rangle\\
		|\nabla r(z)| &<  \frac{r(z) \langle w,  z   \rangle}{|w - w_z|}
	\end{align*}
	by the fact that $w$ is arbitrary, and estimating $|w - w_z|$ as $\sqrt{1^2 - (1-\epsilon)^2}$:
	\begin{align*}
		|\nabla r(z)|  &<\frac{r(z) (1-\epsilon)}{\sqrt{1^2 - (1-\epsilon)^2}}\\
		|\nabla r(z)|  &<r(z)\frac{ (1-\epsilon)}{\sqrt{\epsilon (2-\epsilon)}}
	\end{align*}
	By the preceding lemma, if $ r^2 > C $, then:
	\begin{align*}
		|\nabla r|^2 &\leq \frac{C r^2}{r^2 - C} < C\\
		|\nabla r| &\leq \sqrt{C}
	\end{align*}
	otherwise, 
	\begin{align*}
		|\nabla r(z)|  &<C\frac{ (1-\epsilon)}{\sqrt{\epsilon (2-\epsilon)}}
	\end{align*}
	Hence:
	\begin{align*}
		|\nabla r(z)| < \max\left(\sqrt{C}, C\frac{ (1-\epsilon)}{\sqrt{\epsilon (2-\epsilon)}}\right)
	\end{align*} 
\end{proof}
\section{Expansive flows and ancient solutions}\label{SinestRisaResult}

In this section, roughly following \cite{SinestRisa}, we consider solutions to (\ref{evolutioneq}) which are defined on a larger interval $(T_0, T_1)$, with  $-\infty \leq T_0 \leq 0 < T_1 \leq \infty$. 

We will limit our analysis to a sub-class of solutions to (\ref{evolutioneq}):
\begin{defin}
	We say that a solution to equation (\ref{evolutioneq}) is {\em expansive} if $F<0$. 
\end{defin}
An example of such a flow is the inverse mean curvature flow ($F=-\frac{1}{\kappa_1+\dots + \kappa_n}$), extensively studied in the literature. 
\begin{oss}	
	\em This assumption on $F$ implies that whenever $t<s$, $X_t \subset \mathrm{int}(X_s)$. This is implied by the fact that, in the equation, the time derivative is always an outward pointing non-zero vector.\label{expansive flow remark} In this sense, the hypersurface is expanding in the ambient space.
\end{oss}
\begin{defin}
	Let $ X : M^n \times (T_0, T_1) \to \mathbb{R}^{n+1} $ be an embedded expansive solution to equation (\ref{evolutioneq}). We say that {\em $X$ comes out of a point} if there exists a point $y_\infty$ such that for every $\varepsilon>0$, there exists a time $\tau \in  (T_0, T_1)$ such that $X_\tau \subset B_\varepsilon(y_\infty)$.
\end{defin}

\begin{oss}	
	\em If we take the optimal $\tau$ in the definition above, Remark \ref{expansive flow remark} implies that the function $\tau(\varepsilon)$ mapping $\varepsilon$ to the last time where $X_\tau$ is contained in the ball is non-increasing.
\end{oss} 


In particular, we will show that expansive solutions "coming out of a point" must be expanding spheres.
It is easy to check that homothetically expanding spheres do satisfy the equation: Any spherical solution is completely determined by its radius at time $t$, because the equation is invariant at each point, being the principal curvatures constant: thus, solving the ordinary differential equation $r'(t) = \varphi(r(t))$ completely determines the flow, where $\varphi(r) = -F(\frac{1}{r}, \dots, \frac{1}{r})$.

We say that a solution to the equation is ancient if $T_0=-\infty$. 
In the case of the spherical solution coming out of a point, there exists an ancient solution if and only if
\begin{align*}
	+\infty=(T_1-T_0)=\int_{T_0}^{T_1} 1 dt = \int_{T_0}^{T_1}\frac{r'(t)}{\varphi(r(t))} dt = \int_{r(T_0)}^{r(T_1)}\frac{1}{\varphi(r)} dr = \int_{0}^{c}\frac{1}{\varphi(r)} dr
\end{align*}
\begin{align*}
	\int_{0}^{c}\frac{1}{\varphi(r)} dr =+\infty
\end{align*}
This equation allows us to determine, given $F$, if an ancient solution can exist (even if not coming out of a point). Solutions to (\ref{evolutioneq}) obey an avoidance principle: if a solution is inside another one, it stays inside it at all times. This can be proven similarly to theorem \ref{chow gulliver}: if at some $(x, t)$ this fails and they are tangent, one can apply the maximum principle to show that they coincide in a neighbourhood, which is a contradiction if one takes the first or the last $t$ where this happens. A generic solution, then, will be sandwiched between two expanding sphere solutions, one inside and one outside, and therefore cannot be ancient if no ancient expanding sphere solution exists. 


The idea of ancient solutions was introduced by Richard Hamilton in his work on the Ricci flow. It has since been applied to other geometric flows as well as to other partial differential equations. Ancient solutions are significant because they capture key asymptotic features of the flow and often have unique, rigid properties that distinguish them from other solutions. For instance, in contractive flows, ancient solutions can form complex, non-spherical shapes, whereas in expansive flows, they tend to exhibit strong rigidity, commonly resulting in spherical shapes under certain conditions. These solutions thus help in understanding the geometry and topology of hypersurfaces as they evolve, especially in applications like mean curvature flow and inverse mean curvature flow in mathematics and physics.


Indeed, the property of coming out a point is particularly rigid, as shown in the following result:

\begin{theorem}
	Let $ X : M^n \times (T_0, T_1) \to \mathbb{R}^{n+1} $ be a smooth, closed, embedded expansive solution to equation (\ref{evolutioneq}) coming out of a point. Then it is a family of expanding spheres.\label{pointexpanding}
\end{theorem}

Previous results in geometric flows have shown multiple non-trivial examples of contracting flows sweeping the whole space.
This could therefore be somewhat surprising, because it shows an opposite result, when running the equation in the opposite direction. It can however be explained intuitively by the idea that parabolic flows tend to \textit{smooth things out}: it is thus possible to arrive to a point from a more irregular hypersurface, however the only thing that can ``come out" of a point is something just as symmetric as a point, i.e. a sphere. 

The proof is a relatively simple application of the reflection technique. The outline of the proof in \cite{SinestRisa} is the same as the one below, but using Corollary \ref{reflect a small bit} makes it a bit simpler: 

\begin{proof}
	Fix any hyperplane $\pi$ not passing through $y_\infty$. There is $R>0$ such that $B_{2R}(y_\infty)$ does not intersect it. By definition, there is also a time  $\tau \in  (T_0, T_1)$ such that $X_\tau \subset B_R(y_\infty)$. 
	By Corollary \ref{reflect a small bit}, then, we can reflect strictly $X_t$ up to $\pi$ for any $t>\tau$. 
	
	Now consider a sequence  $\epsilon_n\rightarrow 0$. Up to a subsequence, we can then find a corresponding converging non-increasing sequence $\tau_n\rightarrow \overline{t}\in [T_0, T_1)$ (here note that $\overline{t}$ can be $-\infty$) such that $X_{\tau_n}\subseteq B_{\epsilon_n}(y_\infty)$, therefore in $(\tau_n, T_1)$ we can reflect up to any hyperplane tangent to  $B_{\epsilon_n}(y_\infty)$ in any direction, reasoning like we just did. At time $\overline{t}$, $X_{\overline{t}} \subseteq \cap_r B_{r}(y_\infty) = \{y_\infty\}$, thus we would have a singularity at $\overline{t}$ if  $\overline{t}\in(T_0, T_1)$ and therefore $\overline{t}=T_0$. 
	
	On the other hand, by construction, we can reflect $X_{t}$ strictly about any hyperplane not intersecting $\cap_r B_{r}(y_\infty) = \{y_\infty\}$ at any time $t>\overline{t}=T_0$, hence we can reflect $X_{t}$ strictly up to any hyperplane passing through $y_\infty$, in both directions, at any  time $t\in (T_0, T_1)$. We observe that in the limit, the reflection property becomes non-strict, in the sense that we have to replace the interior of $X$ in definition \ref{strict-reflection-definition} with its closure, therefore $X_{t}$ may touch its reflection at the limit plane, i.e. the one passing through $y_\infty$. Similarly, it cannot be that the other condition is the one causing the strict reflection definition to fail, as the other condition stays instead strict. 
	
	This implies that, taking any hyperplane passing through $y_\infty$ and considering opposite directions for the reflection, $X_{t}$ is symmetric about said hyperplane for any time $t\in (T_0, T_1)$. By Proposition \ref{proposition symmetry conclusion}, then, we conclude that $X_{t}$ must be a ball.	
\end{proof}



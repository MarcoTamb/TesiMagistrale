% !TeX spellcheck = en_GB
\chapter{The Chow-Gulliver Critical Planes Result}

The main result we want to establish in this chapter is theorem \ref{chow gulliver}, a result about critical hyperplanes when applying the method of the moving planes to solution of a large class of non-linear parabolic partial differential equations, and whose proof is somewhat similar to Theorem \ref{Alexandrov theorem}. 

We will first describe the differential equations we are analysing, then prove that they are parabolic, then prove the theorem. 

\section{Class of Equations we analyze}

We consider manifolds $M^n$ embedded in $\R^{n+1}$, i.e. there is an embedding $X_0 : M^n \rightarrow \R^{n+1}$ parametrizing the hypersurface $X_0(M^n)$. 

Let $F:\{(\kappa_1, \dots , \kappa_n)\in \R^n\vert \kappa_1\leq \dots \leq \kappa_n\}\rightarrow \R$ be a $C^1$ function satisfying:

\begin{equation}
	\frac{\partial F}{\partial \kappa_i} > 0 \mathrm{\; for \; all } \; i=1,\dots, n \label{parabolicità}
\end{equation}
and consider the evolution equation 
\begin{align}
	\begin{dcases}
		\frac{\partial X_t}{\partial t} = - F(\kappa_1(x), \dots , \kappa_n(x)) \nu\\
		X(0)= X_0
	\end{dcases} \label{evolutioneq}
\end{align}
where $\nu$ is the outward normal to $X_t(M^n)$ at the point $X_t(x)$ and $\kappa_1\leq \dots \leq \kappa_n$ are the principal curvatures at $X_t(x)$. 


\section{Parabolicity of the differential equation (\ref{evolutioneq})}\label{parabolic}


The condition (\ref{parabolicità}) will  guarantee that equation (\ref{evolutioneq}) is a parabolic equation. This may be confusing, as (\ref{evolutioneq}) does not make it obvious how to apply definition \ref{nonlinearpde}. 

In order to classify a non-linear partial differential equation one has to understand how it behaves ``close to a solution" in the solutions space. We want to prove that very close to any solution, ``moving in any direction", the change in the equation is always a parabolic PDE. This will then tell us that our equation is parabolic, and that the theorems that apply to solutions of parabolic partial differential equations apply to our equation as well. To do so, we are going to ``linearise" the differential equation about a solution.

Like in \cite{huisken}, as $F$ is a symmetric function in the principal curvatures, we may interchangeably take $F$ to be a function of the Weingarten map tensor or of the second fundamental form, and thus we get:

\begin{align*}
	\frac{\partial X_t}{\partial t} &= - F(h_{ij}(X_t)) \nu
\end{align*}

To understand the behaviour close to a solution, we can substitute in our equation $X_t$ with a $X_t+\varepsilon u_t$ to get:

\begin{align}
	\frac{\partial X_t}{\partial t} + \varepsilon\frac{\partial u_t}{\partial t}  &= - F(h_{ij}(X_t+\varepsilon u_t)) \nu_{(X_t+\varepsilon u_t)}\label{linearizingevolutioneq}
\end{align}
where we mean that  $\nu_{(X_t+\varepsilon u_t)}$ is the normal to the perturbed immersion. 
We are interested in the behaviour of this equation for a small $\varepsilon$. This equation when taking the limit for $\varepsilon \rightarrow 0$ is the so-called linearisation of the PDE; we want this PDE to be a parabolic equation to apply our results.

We can use the Weingarten equation (\ref{Weingarten1}) to write the RHS explicitly:

\begin{align*}
	h_{ij}(X_t& + \varepsilon u_t )=-\left\langle v, \nu_{(X_t+\varepsilon u_t)}\right\rangle\; \mathrm{where}\\
	v^\alpha=&\; \frac{\partial^2 X_t^\alpha}{\partial x^i \partial x^j} - \Gamma^k_{ij}\frac{\partial X_t^\alpha}{\partial x^k}+\overline{\Gamma}^\alpha_{\beta \delta}\frac{\partial X_t^\beta}{\partial x^i}\frac{\partial X_t^\delta}{\partial x^k} +\\ %epsilon
	&+\varepsilon\left(\frac{\partial^2 u_t^\alpha}{\partial x^i \partial x^j} - \Gamma^k_{ij}\frac{\partial u_t^\alpha}{\partial x^k}+\overline{\Gamma}^\alpha_{\beta \delta}\left(\frac{\partial X_t^\beta}{\partial x^i}\frac{\partial u_t^\delta}{\partial x^k} + \frac{\partial u_t^\beta}{\partial x^i}\frac{\partial X_t^\delta}{\partial x^k}\right)\right)+\\
	%epsilon squared
	&+\varepsilon^2\left(\overline{\Gamma}^\alpha_{\beta \delta}\frac{\partial u_t^\beta}{\partial x^i}\frac{\partial u_t^\delta}{\partial x^k}\right) \\
	v^\alpha=&\; w +\varepsilon\left(\frac{\partial^2 u_t^\alpha}{\partial x^i \partial x^j} + \mathrm{lower \; order \;  terms}\right) + o(\varepsilon)
\end{align*}
where $h_{ij}(X_t)=-\left\langle w, \nu_{X_t}\right\rangle$.

Putting it all together in the first line:
\begin{align*}
	h_{ij}(X_t + \varepsilon u_t ) &=-\left\langle  w +\varepsilon\left(\frac{\partial^2 u_t}{\partial x^i \partial x^j} + \mathrm{lower \; order \;  terms}\right) + o(\varepsilon), \nu_{(X_t+\varepsilon u_t)}\right\rangle\\
	&= \left\langle  w, \nu_{(X_t+\varepsilon u_t)}\right\rangle -\varepsilon\left\langle  \left(\frac{\partial^2 u_t}{\partial x^i \partial x^j} + \mathrm{lower \; order \;  terms}\right), \nu_{(X_t+\varepsilon u_t)}\right\rangle + o(\varepsilon)\\
	&= h_{ij}(X_t) + \left\langle  w, \nu_{(X_t+\varepsilon u_t)}-\nu_{X_t}\right\rangle -\varepsilon H_{ij} + o(\varepsilon)\\
	&= h_{ij}(X_t) -\varepsilon H_{ij} + o(\varepsilon)
\end{align*}
Were on the last step we are using the fact that $\nu_{(X_t+\varepsilon u_t)}-\nu_{X_t} = O(\varepsilon)$, and as this gets smaller the component of w parallel to the difference also is $O(\varepsilon)$, as $w$ is parallel to $\nu_{X_t}$.
We can then expand F in the RHS of the equation (\ref{linearizingevolutioneq}) to the first order, as it is a $C^1$ function: 
\begin{align*}
	\cancel{\frac{\partial X_t}{\partial t}} + \varepsilon\frac{\partial u_t}{\partial t}  &= - \left(\cancel{F(h_{ij}(X_t)) }
	+ \varepsilon \langle DF, H\rangle + o(\varepsilon)\right)\nu_{(X_t+\varepsilon u_t)}\\
	\varepsilon\frac{\partial u_t}{\partial t}  &= \varepsilon\left(
	\frac{\partial F}{\partial h_{i j}} g^{i k}g^{jl}\left\langle \frac{\partial^2 u_t}{\partial x^k \partial x^l},  \nu_{(X_t+\varepsilon u_t)}\right\rangle + \mathrm{lower \; order \;  terms}\right)\nu_{(X_t+\varepsilon u_t)} + o(\varepsilon)\\
	\frac{\partial u_t}{\partial t}  &= 
	\frac{\partial F}{\partial h_{i j}} g^{i k}g^{jl}\left\langle \frac{\partial^2 u_t}{\partial x^k \partial x^l},  \nu_{(X_t+\varepsilon u_t)}\right\rangle \nu_{(X_t+\varepsilon u_t)} + \mathrm{lower \; order \;  terms} + o(1)
\end{align*}
Letting $\varepsilon\rightarrow 0$, we get to: 

%Here, $\delta h_{ij}(u_t)=\frac{h_{ij}(X_t+\varepsilon u_t)-h_{ij}(X_t)}{\varepsilon}=\frac{h_{ij}(\varepsilon u_t)}{\varepsilon}=h_{ij}(u_t)$ by linearity of the definition of the second fundamental form. 
%\begin{align*}
%	\frac{\partial u_t}{\partial t}  &=  DF(h_{ij}(u_t)) \nu\\
%	\frac{\partial u_t}{\partial t} &= \frac{\partial F}{\partial h_{i j}} g^{i k}g^{jl}h_{k l}(u_t) \nu
%\end{align*}
%
%Using the Weingarten Equations this becomes, implying Einstein's summation convention: 
\begin{align*}
	\frac{\partial u_t}{\partial t} &= \frac{\partial F}{\partial h_{i j}} g^{i k}g^{jl}\left\langle \frac{\partial^2 u_t}{\partial x_k\partial x_l} , \nu \right\rangle \nu+ \mathrm{lower \ order \ terms}
\end{align*}
For the second order term to be positive definite, then, 
\begin{align*}
	\left(\frac{\partial F}{\partial h_{i j}} \right)_{i, j}
\end{align*}
must be positive definite. Or equivalently, as the principal curvatures are the eigenvalues of the matrix $(h_{i j})_{i, j}$,
\begin{align*}
	\frac{\partial F}{\partial \kappa_{i}} > 0
\end{align*}
A more general approach proving that the differential equation is parabolic can be found in \cite{Gerhardt Curvature}. 
\begin{comment}
	{\em Looking more closely at what we are doing, one can prove that it is equivalent to follow this linearisation procedure or to take the derivatives of $F$ with respect to the second order terms.}
\end{comment}

\begin{oss}
	{\em While in this chapter we are using the standard metric of $\R^n$, the proof above is valid using any other metric.}
\end{oss}
\begin{comment}
	contenuto...
	If one considers the F which are linear in the $\kappa_{i}$, this yields:  
	
	\begin{align*}
		\frac{\partial Y_t}{\partial t} &= \left(\frac{\partial F}{\partial\kappa_{i}} \kappa_{i} \right)\nu\\
		\frac{\partial Y_t}{\partial t} &= \frac{\partial F}{\partial\kappa_{i}} o^{ik} h_{kl}o^{li}\,\nu \\ 
		\frac{\partial Y_t}{\partial t} &= \frac{\partial F}{\partial\kappa_{i}}   o^{ik}o^{li}\left\langle \frac{\partial^2 Y_t}{\partial x_k\partial x_l} , \nu \right\rangle \,\nu + \text{lower order terms}
	\end{align*}
	implying summation convention, for an appropriate orthogonal matrix $o_{li}$ that makes $h_{ij}$ diagonal, which makes the equation parabolic if $\frac{\partial F}{\partial \kappa_i}$ is positive. 
	
	{\vspace{10pt}\LARGE \bf [NEED HELP WITH PARABOLICITY!]}
	
	In \cite{huisken}, the paper considers F as a function of the second fundamental form $h_{ij}$, as the principal curvatures are themselves function of it. The following is then said, speaking about "linearisation", while also citing the Weingarten equations:
	\begin{align*}
		\frac{\partial Y_t}{\partial t} &= \frac{\partial F}{\partial h_{i j}} g^{i k}g^{jl}h_{k l} \nu+ \mathrm{lower \ order \ terms}\\
		\frac{\partial Y_t}{\partial t} &= \frac{\partial F}{\partial h_{i j}} g^{i k}g^{jl}\left\langle \frac{\partial^2 Y_t}{\partial x_k\partial x_l} , \nu \right\rangle \nu+ \mathrm{lower \ order \ terms}
	\end{align*}
	Which I did not understand the implication of (especially with respect to maximum principle - there is probably something I am missing due to non-linearity of the equation). 
	
	\begin{theorem}[Parabolicity of the differential equation (\ref{evolutioneq})]
		da scrivere\label{graphparabolic}
	\end{theorem}
	
	Il conto probabilmente è quello in \cite{hamilton}, pagina 262. Info sul nostro caso sono in \cite{huisken}, pagina 50. 
	
	{\vspace{10pt}\LARGE \bf [NEED HELP!]}
	
\end{comment}

\section{An existence result}
In \cite{huisken} one finds the following comforting existence result for the class of equations we are analysing under very broad hypothesis. The same paper also includes a proof of the result with some more restrictive hypotheses. A complete proof is quite more involved. We will not use it, nor prove it, but it is included here for completeness and peace of mind: 

\begin{theorem}[Short term existence of a solution for (\ref{evolutioneq})]
	Suppose $X_0 : M^n \rightarrow \R^{n+1}$ is a smooth, closed hypersurface in $\R^{n+1}$, such that (\ref{parabolicità}) holds at all points in $X_0$, i.e. for all the values of the principal curvatures $\kappa_{i}$ realized at some point on $X_0$. Then, (\ref{evolutioneq}) has a smooth solution, at least on some short time interval $[0, T)$, $T > 0$.  \label{existence}
\end{theorem}

A much more extended analysis of the problem of the existence of solution to the equation can be found in \cite{Gerhardt Curvature}. 


\section{Local representation as a graph of a solution of (\ref{evolutioneq})}
\label{representation as graph}
As a first step, from the Corollary \ref{localgraphcorollary}, we can establish the following:

\begin{theorem}[Local representation as a graph of a solution of (\ref{evolutioneq})]
	Let $X^n$ be a submanifold $X^n\subset\R^{n+1}$ and let $F:X^n\times(0, T)\rightarrow \R^{n+1}$ be a solution of (\ref{evolutioneq}). Also let $t\in(0, T)$ and $x\in F(X^n, t)$.
	Then there exists a neighbourhood of $(x, t)$, $U\subset F(X^n\times(0, T))$, and a smooth function $f: T_{x} F(X^n, t)\times(0, T)\rightarrow \R$ such that any $(x_0, t_0)\in U$ can be expressed as 
	\begin{align*}
		x_0= p +f(p, t_0) \nu 
	\end{align*}
	where $\nu$ is a vector normal to $T_{x} F(X^n, t)$, for an appropriate point $p\in T_x F(X^n, t)$. \label{localgraph}
\end{theorem}
\begin{comment}
	{\vspace{10pt}\LARGE \bf [NON FUNZIONA! Difficile (impossibile?)dimostrare che è smooth]}
	With a reasoning similar to \ref{localgraphclassic} and \ref{localgraphcorollary} one can prove that given any plane, we can represent locally a manifold $M\subset\R^{n+1}$ at $x\in M$ as a graph on that plane, i.e. as $x_0= p + f(p) \nu$ for $\nu$ the vector orthogonal to the plane, as long as $\nu\notin T_xM$. 
	The function which associates to $t\in [0,T)$ the unit vector orthogonal to $T_{F(x, t)}F(X, t)$ is continuous, as $F$ is smooth and the vector can be computed from the derivatives of the local maps of the manifold. Thus we can find a neighbourhood $(a,b)$ of $ t_0$ such that for $t \in (a,b)$ the normal vector is not in $T_x F(X^n, t)$. In $(a,b)$ we can represent $F(X)
\end{comment}


\begin{proof}
	We can consider the image of $F:X^n\times(0, T)\rightarrow \R^{n+1}$ as a manifold in $\R^{n+2}$ by considering $G:=(F(x, t), t)$. Moreover $\frac{\partial G_t}{\partial e_j}\equiv 0$ for all possible vectors of the canonical basis of $\R^{n+1}\times \R$ except for the one corresponding to the time coordinate, where it is $1$. Also, $\frac{\partial G_i}{\partial e_j}\equiv \frac{\partial F_i}{\partial e_j}$ and the first $n$ coordinates of  $\frac{\partial G}{\partial t}$ form a vector normal to $T_x X^n$ by (\ref{evolutioneq}).  Thus, the tangent space of $\mathrm{Im}(G)$ is $T_x X^n \times \{0\}\oplus  \mathrm{span}\langle (\nu, 1)\rangle$ and we can apply corollary  \ref{localgraphcorollary} to $\mathrm{Im}(G)$ to get a function $\tilde{f}$ such that 
	\begin{align*}
		(x_0, t)= \left[(p, 0) +  (s\nu, s)\right] + \tilde{f}(p, s)(\nu, -\Vert \nu\Vert)
	\end{align*}
	as $(\nu, -\Vert \nu\Vert)$ is the vector orthogonal to  $T_x X^n \times \{0\}\oplus  \mathrm{span}\langle (\nu, 1)\rangle$. Let $\sigma_p:I\subset\R\rightarrow\R$ be the function that associates to $t$ the appropriate $s$ in the expression above.  Projecting to the first $n+1$ coordinates and calling $f(p, t)=\sigma_p(t)+\tilde{f}(p, \sigma_p(t))$ one gets:
	\begin{align*}
		x_0= p + f(p, t)\nu 
	\end{align*}
	which is our thesis, as long as $\sigma_p(t)$ is smooth. This is indeed the case, as the graph function $\Gamma_f:x\mapsto(x, f(x))$ is smooth for any smooth function $f$ and has a smooth inverse (and thus, the inverse $(x_0, t)\mapsto ((p, 0) +  (s\nu, s))$ is smooth).
\end{proof}
One can show through direct calculation (see \cite{mantegazza}, Exercise 1.1.2) that, if an immersed hypersurface $\phi : M\rightarrow \R^{n+1}$ is locally the graph of a function $f:\R^n \rightarrow \R$ (i.e., locally, $(x, f(x))=\phi$), then: 
\begin{align*}
	g_{ij}&=\delta_{ij}+ \frac{\partial f}{\partial x_i} \frac{\partial f}{\partial x_j}\\
	\nu&= -\frac{(\nabla f, -1)}{\sqrt{1+|\nabla f|^2}}\\
	h_{ij}&= -\frac{H_{ij}}{\sqrt{1+|\nabla f|^2}}
\end{align*}
Where the matrix $(H_{ij})_{ij}$ is the hessian of $f$. Thus, if one considers the principal curvatures, they are closely related to the eigenvalues of the hessian of $f$. 




\section{The Moving Planes Method and the Chow result}

We will now present the Chow-Gulliver result, presented more or less as in paragraph 2 in \cite{Chow}.
Suppose that we have a hypersurface embedded in $\R^{n+1}$ evolving according to equation (\ref{evolutioneq}). For a fixed time $t$, we can apply the method of the moving planes as described in section \ref{method moving planes}: we can take parallel hyperplanes $\pi_{v,s}$ orthogonal to $v$ intersecting $X$, and consider the reflection  $X_{v,s}^\pi$. There will be a hyperplane $\pi_{v,m_v}$ where $X$ and $X_{v,s}^\pi$ are tangent. As the hypersurface evolves, we may wonder how this critical threshold changes over time. If it behaves in a predictable way, we can hope to use the technique on the evolving manifold, otherwise, it may be hopeless if the critical plane moves back and forth multiple times. In the next section, we are going to prove a result in this general direction, to show a form of "regularity" in this sense. First, however, we need to introduce a marginally stricter definition for the concept of "reflecting inside itself". 



Let $\pi$ be a hyperplane in $\R^{n+1}$. We may assume $\pi$ orthogonal to a unit vector $v\in\R^{n+1}$, i.e. $\langle x, v\rangle= C$ for all $x\in \pi$ for some constant $C$. In our notation for the method of moving planes, assuming we pick the origin as a starting point, this means that $\pi = \pi_{v, C}$. 

Then, $\R^{n+1}$ is divided by $\pi$ into two half-spaces, which we will name 
\begin{align*}
H^+(\pi)&=\{x \in \R^{n+1} : \langle x, v\rangle > C\}= \bigcup_{s>C} \pi_{v, s} \;\;\mathrm{and}\\
H^-(\pi)&=\{x \in \R^{n+1} : \langle x, v\rangle < C\}= \bigcup_{s<C} \pi_{v, s}.
\end{align*} 

\begin{defin}
	We say {\em we can reflect $X: M^n\rightarrow \R^{n+1}$ strictly with respect to $\pi$} if both:
	\begin{itemize}
		\item $X^\pi\cap H^-(\pi)\subset \mathrm{int}(X)\cap H^-(\pi)$ where $X^\pi$ is the reflection of $X$ about $\pi$ and $\mathrm{int}(X)$ is the region inside $X$.
		\item $V\notin T_xM$ for all $x\in M^n \cap\pi$
	\end{itemize} 
\end{defin}
This fundamentally means that the reflection of one of the halves of $X$ on the other side of $\pi$ is contained in the region inside $M^n$ and the tangent spaces of $X$ and of the half-reflection do not form a ninety degree angle with $\pi$, at all points on $\pi\cap X$. As the two tangent spaces are one the reflection of the other, this means that they do not coincide.   
\begin{defin}
	We say {\em we can reflect $X: M^n\rightarrow \R^{n+1}$ strictly up to $(\pi,v)$} if we can reflect $M^n$ strictly with respect to $\pi_{v, s}$ for all hyperplanes $\pi_{v, s}$ such that $s<C$.  
\end{defin}

The key idea of the main result in the next section is as follows: suppose we have an embedded smooth hypersurface $X$ evolving according to (\ref{evolutioneq}) and a fixed hyperplane $\pi$, intersecting $X$. Suppose that, at some time $t$, $X$ and  $X_\pi$ touch outside of $\pi$. We can consider $X$ and  $X_\pi$ as local graphs over the same hyperplane $\pi$, and we can show that these function evolve according to the same differential equation. Using the strong maximum principle and the Hopf boundary point lemma, then, one can conclude that the two functions coincide, and have been coinciding up until that point. We can then conclude that if  $X$ and  $X_\pi$ only touch in $X\cap\pi$ at the beginning of the evolution, then they will never touch elsewhere.  


\section{The Chow-Gulliver result}

The main theorem is the following: 

\begin{theorem}[Chow-Gulliver]\label{chow gulliver}
	Let $X:M^n\times [0,T) \rightarrow \R^{n+1}$ be a $C^2$ solution to equation (\ref{evolutioneq}). Then, if we can reflect $X(M^n, 0)=X_0$ strictly with respect to $\pi$, then for all $t\in [0,T)$ we can reflect $X(M^n, t)=X_t$ strictly with respect to $\pi$. 
\end{theorem}

\begin{proof}
	By contradiction, suppose that there is a time $t$ such that the thesis is false, and that it is the smallest such $t$. Then, for all $\tau \in [0,t)$, $X_{\tau,\pi}\cap H^-(\pi)\subset \mathrm{int}(X_{\tau})\cap H^-(\pi)$; the unit vector orthogonal to $\pi$, $V$, is such that $V\notin T_xX_\tau$ for all $x\in X_\tau\cap \pi$ and $\tau \in [0,t)$; and either of the conditions fails at $t$, i.e. either: 
	\begin{itemize}
		\item[(i)] $X_{t,\pi}\cap H^-(\pi)\cap X_{t}\neq \emptyset$
		\item[(ii)] $V\in T_xX_t$  for some $x\in\pi$. 
	\end{itemize} 
	
	(i) Suppose the first case is true. Then, there exists $x_0 \in X_{t,\pi}\cap H^-(\pi)\cap X_{t}$ such that at $x_0$ the two manifolds are tangent. \\
	We can take a neighbourhood of $(x_0, t)\in X_{t}\times \R$ such that  both $X_{t,\pi}$ and $X_{t}$ are graphs over $T_{x_0}X_{t}$ by \ref{localgraph}. \\
	We can explicitly write the functions $f:U\times (t-\varepsilon, t+\varepsilon)\rightarrow X_t$, where $U\subset T_{x_0}X_{t}$, and the corresponding $f_\pi$ for $X_{t,\pi}$. 
	We can also write 
	\begin{align*}
		f \; : \; (x, t) &\mapsto x+\tilde{f}(x, t)\nu \\
		f_\pi \; : \; (x, t) &\mapsto x+\tilde{f}_\pi(x, t)\nu 
	\end{align*}
	for appropriate functions $\tilde{f}:U\times (t-\varepsilon, t+\varepsilon)\rightarrow \R$ and $\tilde{f}_\pi:U\times (t-\varepsilon, t+\varepsilon)\rightarrow \R$, where $\nu$ is a fixed unit vector normal to $T_{x_0}X_{t}$.  $\tilde{f}$ and $\tilde{f_\pi}$ are solutions to the same second order PDE, which is parabolic by what was discussed in paragraph \ref{parabolic}, hence we can apply Proposition \ref{firstapplication} to conclude that $\tilde{f}\equiv\tilde{f_\pi}$, and thus $X_{t,\pi}$ and $X_{t}$ coincide in a neighbourhood of $(x, t)$, a contradiction as we assumed that $t$ is the first $t$ where the flows touch.
	
	(ii) Suppose instead that $V\in T_xX_t$  for some $t\in [0, t)$ and some $x\in X_t\cap \pi$. Then $T_xX_t= T_xX_{t, \pi}$ and in a neighbourhood of $(x, t)$ both $X_t$ and $X_{t, \pi}$ are graphs of two smooth functions over $T_xX_t$ by \ref{localgraph}, i.e. again
	\begin{align*}
		f \; : \; (x, t) &\mapsto x+\tilde{f}(x, t)\nu \\
		f_\pi \; : \; (x, t) &\mapsto x+\tilde{f}_\pi(x, t)\nu 
	\end{align*} 
	Moreover, in $\overline{H^-(\pi)}$, $f_\pi\geq f$, because $M^n_\pi\cap H^-(\pi)\subset \mathrm{int}(M^n)\cap H^-(\pi)$. Finally, $f(x, t)=f_\pi (x, t)$, hence $f_\pi-f (x, t)=0$, and thus  $(x, t)$ is a minimum point on the boundary for $f_\pi-f$. Also, we must have
	\begin{align*}
		\frac{\partial f}{\partial V}(x,t)=\frac{\partial f_\pi}{\partial V}(x,t)
	\end{align*}
	because the graphs are both tangent to $T_xX_t$, and $V$ here is the outward pointing normal to the boundary by definition of the reflection. Thus, 
	\begin{align*}
		\frac{\partial (f- f_\pi)}{\partial V}(x,t)=0
	\end{align*}
	But we must have 
	\begin{align*}
		\frac{\partial (f- f_\pi)}{\partial V}(x,t)>0
	\end{align*}
	at a minimum on the boundary by Proposition \ref{secondapplication}, a contradiction.  
\end{proof}


\section{Some corollaries of the result (da scrivere)} 
In this section we collect a number of corollaries to the main result above. This first one is an immediate consequence of the main result:
\begin{cor}
	Let $X:M^n\times [0,T) \rightarrow \R^{n+1}$ be a $C^2$ embedded solution to \ref{evolutioneq}. Then, if we can reflect $X_0$ strictly up to $(\pi_{v,C},v)$, then for all $t\in [0,T)$ $v\notin T_xX_t$ for all $x\in X_t\cap\overline{H^+(\pi)}$. In particular,  $ X_t\cap\overline{H^+(\pi)}$ is a graph over $\pi$ for all $t\in [0,T)$.
\end{cor}




Another immediate consequence of Theorem \ref{chow gulliver} is the following:
\begin{cor}
	Let $X:M^n\times [0,T) \rightarrow \R^{n+1}$ be a $C^2$ solution to \ref{evolutioneq}. Then, if we can reflect $X_0$ strictly up to $(\pi_{v,C},v)$, then for all $t\in [0,T)$ we can reflect $X(M, t)$ strictly up to $(\pi_{v,C},v)$.  
\end{cor}
\begin{proof}
	The hypothesis of the theorem are true for each $\pi_K$ in the definition, thus we can reflect strictly with respect to each $\pi_K$ for all $t\in[0,T)$, and thus we can reflect $X(M, t)$ strictly up to $(\pi_{v,C},v)$.
\end{proof}

 Furthermore, it is clear that for every direction $v$ there exists a hyperplane $\Pi$ perpendicular to $v$ such that we can reflect $X_0$ up to $(\Pi, v)$ and $\Pi$ intersects the interior of $X_0$. To be more precise, for every direction $v$ there exists a hyperplane $\Pi_0^v$ tangent to $X_0$ and such that $X_0\cap H^+(\Pi_0^v)=\emptyset$; we can find a plane $\Pi_0^v +\epsilon v$ such that we can reflect $X_0$ up to $(\Pi_0^v +\epsilon v, v)$ and $\Pi_0^v +\epsilon v$ intersects the interior of $X_0$. Thus, we obtain the following by compactness:


\begin{cor}
	Let $X:M^n\times [0,T) \rightarrow \R^{n+1}$ be a $C^2$ embedded solution to \ref{evolutioneq}. There exists $\varepsilon>0$ depending only on $X_0$ such that for all $t\in[0, T)$ we can reflect $X_t$ up to $(\Pi_0^v +\epsilon v, v)$ for every $v \in S^n$. In particular, if $X_0 \in B_R (C)$, then we can always reflect $X_t$ up to $(\Pi, v)$ whenever $H^+(\Pi)\cap B_{R-\varepsilon}(C)=\emptyset$.
\end{cor}
In other words, we can always reflect a little $\varepsilon$ in any direction, uniformly. We can use the fact that we can always reflect about a plane outside a sphere containing $X_0$ to prove the following estimate:




\begin{cor}
	Let $X:M^n\times [0,T) \rightarrow \R^{n+1}$ be a $C^2$ embedded solution to \ref{evolutioneq}. There exists $C>0$ depending only on $X_0$ such that for all $t\in[0, T)$: 
	\begin{align*}
		\max_{x\in X_t} |x| - \min_{x\in X_t} |x| < C
	\end{align*}
\end{cor}
\begin{proof}
	We can reflect $X_0\subset B_R(0)$ up to any plane tangent to  $B_R(0)$, i.e. any plane $\pi_{v, K}$ for any $K\geq R$ and $v$ unit vector, i.e.  $\pi_{v, K}=\{p \in \R^{n+1} : \langle p, v \rangle = K\}$. Let $x_1, x_2\in X_t$ such that $|x_1|=\min_{x\in X_t} |x|$ and  $|x_2|=\max_{x\in X_t} |x|$. Let $v=\frac{x_2-x_1}{\vert x_2-x_1 \vert}$: we can reflect $X_t$ up to $(\pi_{v, R}, v)$ by theorem \ref{chow gulliver}, therefore $\mathrm{dist}(x_2, \pi_{v, R}) < \mathrm{dist}(x_1, \pi_{v, R})$, or in other words:
	\begin{align*}
		 \left\langle x_2, \frac{x_2-x_1}{\vert x_2-x_1 \vert} \right\rangle - R &< 
		 \left\langle x_1, \frac{x_1-x_2}{\vert x_2-x_1 \vert} \right\rangle + R\\
		 \frac{|x_2|^2}{\vert x_2-x_1 \vert} + \cancel{\frac{\left\langle x_2, x_1\right\rangle}{\vert x_2-x_1 \vert}}  - R &< 
		 \frac{|x_1|^2}{\vert x_2-x_1 \vert} - \cancel{\frac{\left\langle x_2, x_1\right\rangle}{\vert x_2-x_1 \vert}}  + R\\
		 \frac{|x_2|^2 - |x_1|^2}{\vert x_2-x_1 \vert} &< 
		  2R\\
		  |x_2|^2 - |x_1|^2 &< 
		  2R \vert x_2-x_1 \vert \leq 4R |x_2|\\
		  |x_2|^2 &<|x_1|^2 + 4R |x_2|\\
		  |x_2| &< |x_1| \frac{|x_1|}{|x_2|} + 4R < |x_1| + 4R\\
		  |x_2| -|x_1| &<4R
	\end{align*}
\end{proof}


Lastly: 
\begin{cor}
	Let $X:M^n\times [0,T) \rightarrow \R^{n+1}$ be a $C^2$ embedded solution to \ref{evolutioneq}. Let $s_v:[0,T) \rightarrow I$ be such that 
	\begin{align*}
		s_v(t)= \sup\left\{s \in I \;|\;  \; \mathrm{we \; can \; reflect \;} X_t \mathrm{ \; strictly \; up \; to }\; (\pi_{v, s}, v) \right\}.
	\end{align*}
	Then $s_v(t)$ is a non-decreasing function. Also, if $X_0$ is compact, the limit
	\begin{align*}
		\lim_{t\rightarrow T^-} s_v(t)
	\end{align*}
	exists and is finite.
\end{cor}
\begin{proof}
	When taking $X_t$ as the starting manifold, the hypothesis of the theorem are still true, therefore we can reflect about $\pi_{c, v}$  for all $c<s_v(t)$ at all subsequent times. Thus $s_v(t)$ is non-decreasing. $\lim_{t\rightarrow T^-} s_v(t) = \sup s_v(t)$, therefore the limit exists. Also, if $X_0$ is bounded, there exists $R>0$ such that $X_0\subset B_R(0)$, therefore we can reflect $X_0$ strictly about any plane non intersecting $B_R(0)$, as it does not touch $X_0$, and therefore we can also reflect $X_t$ strictly about the same planes by theorem \ref{chow gulliver}. At the same time, there exists a plane such that $X_t$ cannot be reflected strictly about it, because there will always be a straight line parallel to $v$ intersecting $X_t$ in multiple points, and we can always consider the plane orthogonal to $v$ passing through their midpoint, about which $X_t$ cannot be reflected strictly, and $s_v(t) \neq +\infty$. Therefore  $s_v(t)\in [-R, R]$, and the limit above is finite. 
\end{proof}


\section{Applying the result to find gradient estimates (da scrivere)}

{\LARGE \textbf{[normal displacement estimate (2.4) - DA SCRIVERE]}}

{\LARGE \textbf{[support function for convex hypersurfaces (2.5) - DA SCRIVERE]}}

{\LARGE \textbf{[gradient estimate for starshaped surfaces (2.7) - DA SCRIVERE]}}

{\LARGE \textbf{[ (2.9b) DA SCRIVERE]}}


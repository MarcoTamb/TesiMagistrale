% !TeX spellcheck = en_GB
\chapter{The Chow Critical Planes Result}

{\vspace{10pt}\LARGE \bf [FIX PARAGRAPH]}
The main result we want to establish in this chapter is theorem \ref{reflection}, a result in Geometric Analysis and the main topic of the thesis. Before we can tackle it, one has to go first through a couple of classical results. 


\noindent{\vspace{10pt}\LARGE \bf [READ CHAPTER AND DOUBLE CHECK IT]}
\section{Class of Equations we analyze}

We consider manifolds $M^n$ embedded in $\R^{n+1}$, i.e. there is an embedding $X_0 : M^n \rightarrow \R^{n+1}$ parametrizing the hypersurface $X_0(M^n)$. 

Let $F:\{(\kappa_1, \dots , \kappa_n)\in \R^n\vert \kappa_1\leq \dots \leq \kappa_n\}\rightarrow \R$ be a $C^1$ function satisfying:

\begin{equation}
	\frac{\partial F}{\partial \kappa_i} > 0 \mathrm{\; for \; all } \; i=1,\dots, n \label{parabolicità}
\end{equation}
and consider the evolution equation 
\begin{align}
	\begin{dcases}
		\frac{\partial X_t}{\partial t} = F(\kappa_1(x), \dots , \kappa_n(x)) \nu\\
		X(0)= X_0
	\end{dcases} \label{evolutioneq}
\end{align}
where $\nu$ is the inward normal to $X_t(M^n)$ at the point $X_t(x)$ and $\kappa_1\leq \dots \leq \kappa_n$ are the principal curvatures at $X_t(x)$. 


\section{Parabolicity of the differential equation (\ref{evolutioneq})}\label{parabolic}


The condition (\ref{parabolicità}) will  guarantee that equation (\ref{evolutioneq}) is a parabolic equation. This may be confusing, as (\ref{evolutioneq}) does not make it obvious how to apply definition \ref{nonlinearpde}. 

{\vspace{10pt}\LARGE \bf [FIX PARAGRAPH]}
In order to classify a non-linear partial differential equation one has to understand how it behaves ``close to a solution" in the solutions space. We want to prove that very close to any solution, ``moving in any direction", the change in the equation is always a parabolic PDE. This will then tell us that our equation is parabolic, and that the theorems that apply to solutions of parabolic partial differential equations apply to our equation as well. 

Like in \cite{huisken}, as $F$ is a symmetric function in the principal curvatures, we may interchangeably take $F$ to be a function of the Weingarten map tensor or of the second fundamental form, and thus we get:

\begin{align*}
	\frac{\partial X_t}{\partial t} &= F(h_{ij}(X_t)) \nu
\end{align*}

To understand the behaviour close to a solution, we can substitute in our equation $X_t$ with a $X_t+\varepsilon u_t$ to get:

\begin{align}
	\frac{\partial X_t}{\partial t} + \varepsilon\frac{\partial u_t}{\partial t}  &= F(h_{ij}(X_t+\varepsilon u_t)) \nu_{(X_t+\varepsilon u_t)}\label{linearizingevolutioneq}
\end{align}
where we mean that  $\nu_{(X_t+\varepsilon u_t)}$ is the normal to the perturbed immersion. 
We are interested in the behaviour of this equation for a small $\varepsilon$. This equation when taking the limit for $\varepsilon \rightarrow 0$ is the so-called linearisation of the PDE; we want this PDE to be a parabolic equation to apply our results.

We can use the Weingarten equation (\ref{Weingarten1}) to write the RHS explicitly:

\begin{align*}
	h_{ij}(X_t& + \varepsilon u_t )=-\left\langle v, \nu_{(X_t+\varepsilon u_t)}\right\rangle\; \mathrm{where}\\
	v^\alpha=&\; \frac{\partial^2 X_t^\alpha}{\partial x^i \partial x^j} - \Gamma^k_{ij}\frac{\partial X_t^\alpha}{\partial x^k}+\overline{\Gamma}^\alpha_{\beta \delta}\frac{\partial X_t^\beta}{\partial x^i}\frac{\partial X_t^\delta}{\partial x^k} +\\ %epsilon
	&+\varepsilon\left(\frac{\partial^2 u_t^\alpha}{\partial x^i \partial x^j} - \Gamma^k_{ij}\frac{\partial u_t^\alpha}{\partial x^k}+\overline{\Gamma}^\alpha_{\beta \delta}\left(\frac{\partial X_t^\beta}{\partial x^i}\frac{\partial u_t^\delta}{\partial x^k} + \frac{\partial u_t^\beta}{\partial x^i}\frac{\partial X_t^\delta}{\partial x^k}\right)\right)+\\
	%epsilon squared
	&+\varepsilon^2\left(\overline{\Gamma}^\alpha_{\beta \delta}\frac{\partial u_t^\beta}{\partial x^i}\frac{\partial u_t^\delta}{\partial x^k}\right) \\
	v^\alpha=&\; w +\varepsilon\left(\frac{\partial^2 u_t^\alpha}{\partial x^i \partial x^j} + \mathrm{lower \; order \;  terms}\right) + o(\varepsilon)
\end{align*}
where $h_{ij}(X_t)=-\left\langle w, \nu_{(X_t+\varepsilon u_t)}\right\rangle$. Putting it all together in the first line:
\begin{align*}
	h_{ij}(X_t + \varepsilon u_t ) &=-\left\langle  w +\varepsilon\left(\frac{\partial^2 u_t}{\partial x^i \partial x^j} + \mathrm{lower \; order \;  terms}\right) + o(\varepsilon), \nu_{(X_t+\varepsilon u_t)}\right\rangle\\
	&= h_{ij}(X_t) -\varepsilon\left\langle  \left(\frac{\partial^2 u_t}{\partial x^i \partial x^j} + \mathrm{lower \; order \;  terms}\right), \nu_{(X_t+\varepsilon u_t)}\right\rangle + o(\varepsilon)\\
	&= h_{ij}(X_t) -\varepsilon H + o(\varepsilon)
\end{align*}
We can then expand F in the RHS of the equation (\ref{linearizingevolutioneq}) to the first order, as it is a $C^1$ function: 

\noindent{\vspace{10pt}\LARGE \bf [PROBABILMENTE HAI PERSO UN \\ PRODOTTO SCALARE PER LA STRADA]}
\begin{align*}
	\cancel{\frac{\partial X_t}{\partial t}} + \varepsilon\frac{\partial u_t}{\partial t}  &= \left(\cancel{F(h_{ij}(X_t)) }
	- \varepsilon DF (H) + o(\varepsilon)\right)\nu_{(X_t+\varepsilon u_t)}\\
	\varepsilon\frac{\partial u_t}{\partial t}  &= - \varepsilon\left(
	\frac{\partial F}{\partial h_{i j}} g^{i k}g^{jl}\frac{\partial^2 u_t}{\partial x^i \partial x^j}  + \mathrm{lower \; order \;  terms}\right)\nu_{(X_t+\varepsilon u_t)} + o(\varepsilon)\\
	\frac{\partial u_t}{\partial t}  &= 
	- \frac{\partial F}{\partial h_{i j}} g^{i k}g^{jl}\frac{\partial^2 u_t}{\partial x^i \partial x^j} \nu_{(X_t+\varepsilon u_t)} + \mathrm{lower \; order \;  terms} + o(1)
\end{align*}
Using a similar line of reasoning for $\nu_{(X_t+\varepsilon u_t)}$ to only keep the highest order terms, using \ref{Weingarten2},  and letting $\varepsilon\rightarrow 0$, we get to: 

%Here, $\delta h_{ij}(u_t)=\frac{h_{ij}(X_t+\varepsilon u_t)-h_{ij}(X_t)}{\varepsilon}=\frac{h_{ij}(\varepsilon u_t)}{\varepsilon}=h_{ij}(u_t)$ by linearity of the definition of the second fundamental form. 
%\begin{align*}
%	\frac{\partial u_t}{\partial t}  &=  DF(h_{ij}(u_t)) \nu\\
%	\frac{\partial u_t}{\partial t} &= \frac{\partial F}{\partial h_{i j}} g^{i k}g^{jl}h_{k l}(u_t) \nu
%\end{align*}
%
%Using the Weingarten Equations this becomes, implying Einstein's summation convention: 
\begin{align*}
	\frac{\partial u_t}{\partial t} &= \frac{\partial F}{\partial h_{i j}} g^{i k}g^{jl}\left\langle \frac{\partial^2 u_t}{\partial x_k\partial x_l} , \nu \right\rangle \nu+ \mathrm{lower \ order \ terms}
\end{align*}
For the second order term to be positive definite, then, 
\begin{align*}
	\left(\frac{\partial F}{\partial h_{i j}} \right)_{i, j}
\end{align*}
must be positive definite. Or equivalently, as the principal curvatures are the eigenvalues of the matrix $(h_{i j})_{i, j}$,
\begin{align*}
	\frac{\partial F}{\partial \kappa_{i}} > 0
\end{align*}

\begin{comment}
	{\em Looking more closely at what we are doing, one can prove that it is equivalent to follow this linearisation procedure or to take the derivatives of $F$ with respect to the second order terms.}
\end{comment}

\begin{oss}
	{\em While in this chapter we are using the standard metric of $\R^n$, the proof above is valid using any other metric.}
\end{oss}
\begin{comment}
	contenuto...
	If one considers the F which are linear in the $\kappa_{i}$, this yields:  
	
	\begin{align*}
		\frac{\partial Y_t}{\partial t} &= \left(\frac{\partial F}{\partial\kappa_{i}} \kappa_{i} \right)\nu\\
		\frac{\partial Y_t}{\partial t} &= \frac{\partial F}{\partial\kappa_{i}} o^{ik} h_{kl}o^{li}\,\nu \\ 
		\frac{\partial Y_t}{\partial t} &= \frac{\partial F}{\partial\kappa_{i}}   o^{ik}o^{li}\left\langle \frac{\partial^2 Y_t}{\partial x_k\partial x_l} , \nu \right\rangle \,\nu + \text{lower order terms}
	\end{align*}
	implying summation convention, for an appropriate orthogonal matrix $o_{li}$ that makes $h_{ij}$ diagonal, which makes the equation parabolic if $\frac{\partial F}{\partial \kappa_i}$ is positive. 
	
	{\vspace{10pt}\LARGE \bf [NEED HELP WITH PARABOLICITY!]}
	
	In \cite{huisken}, the paper considers F as a function of the second fundamental form $h_{ij}$, as the principal curvatures are themselves function of it. The following is then said, speaking about "linearisation", while also citing the Weingarten equations:
	\begin{align*}
		\frac{\partial Y_t}{\partial t} &= \frac{\partial F}{\partial h_{i j}} g^{i k}g^{jl}h_{k l} \nu+ \mathrm{lower \ order \ terms}\\
		\frac{\partial Y_t}{\partial t} &= \frac{\partial F}{\partial h_{i j}} g^{i k}g^{jl}\left\langle \frac{\partial^2 Y_t}{\partial x_k\partial x_l} , \nu \right\rangle \nu+ \mathrm{lower \ order \ terms}
	\end{align*}
	Which I did not understand the implication of (especially with respect to maximum principle - there is probably something I am missing due to non-linearity of the equation). 
	
	\begin{theorem}[Parabolicity of the differential equation (\ref{evolutioneq})]
		da scrivere\label{graphparabolic}
	\end{theorem}
	
	Il conto probabilmente è quello in \cite{hamilton}, pagina 262. Info sul nostro caso sono in \cite{huisken}, pagina 50. 
	
	{\vspace{10pt}\LARGE \bf [NEED HELP!]}
	
\end{comment}

\section{An existence result}
The paper from Huisken and Polden \cite{huisken} states the following comforting existence result for the class of equations we are analysing under very broad hypothesis. The same paper also includes a proof of the result with some more restrictive hypotheses. A complete proof is quite more involved. We will not use it, nor prove it, but it is included here for completeness and peace of mind: 

\begin{theorem}[Short term existence of a solution for (\ref{evolutioneq})]
	Suppose $X_0 : M^n \rightarrow \R^{n+1}$ is a smooth, closed hypersurface in $\R^{n+1}$, such that (\ref{parabolicità}) holds at all points in $X_0$, i.e. for all the values of the principal curvatures $\kappa_{i}$ realized at some point on $X_0$. Then, (\ref{evolutioneq}) has a smooth solution, at least on some short time interval $[0, T)$, $T > 0$.  \label{existence}
\end{theorem}



\section{Local representation as a graph of a solution of (\ref{evolutioneq})}
As a first step, from the Corollary \ref{localgraphcorollary}, we can establish the following:

\begin{theorem}[Local representation as a graph of a solution of (\ref{evolutioneq})]
	Let $X^n$ be a submanifold $X^n\subset\R^{n+1}$ and let $F:X^n\times(0, T)\rightarrow \R^{n+1}$ be a solution of (\ref{evolutioneq}). Also let $t\in(0, T)$ and $x\in F(X^n, t)$.
	Then there exists a neighbourhood of $(x, t)$, $U\subset F(X^n\times(0, T))$, and a smooth function $f: T_{x} F(X^n, t)\times(0, T)\rightarrow \R$ such that any $(x_0, t_0)\in U$ can be expressed as 
	\begin{align*}
		x_0= p +f(p, t_0) \nu 
	\end{align*}
	where $\nu$ is a vector normal to $T_{x} F(X^n, t)$, for an appropriate point $p\in T_x F(X^n, t)$. \label{localgraph}
\end{theorem}
\begin{comment}
	{\vspace{10pt}\LARGE \bf [NON FUNZIONA! Difficile (impossibile?)dimostrare che è smooth]}
	With a reasoning similar to \ref{localgraphclassic} and \ref{localgraphcorollary} one can prove that given any plane, we can represent locally a manifold $M\subset\R^{n+1}$ at $x\in M$ as a graph on that plane, i.e. as $x_0= p + f(p) \nu$ for $\nu$ the vector orthogonal to the plane, as long as $\nu\notin T_xM$. 
	The function which associates to $t\in [0,T)$ the unit vector orthogonal to $T_{F(x, t)}F(X, t)$ is continuous, as $F$ is smooth and the vector can be computed from the derivatives of the local maps of the manifold. Thus we can find a neighbourhood $(a,b)$ of $ t_0$ such that for $t \in (a,b)$ the normal vector is not in $T_x F(X^n, t)$. In $(a,b)$ we can represent $F(X)
\end{comment}


\begin{proof}
	We can consider the image of $F:X^n\times(0, T)\rightarrow \R^{n+1}$ as a manifold in $\R^{n+2}$ by considering $G:=(F(x, t), t)$. Moreover $\frac{\partial G_t}{\partial e_j}\equiv 0$ for all possible vectors of the canonical basis of $\R^{n+1}\times \R$ except for the one corresponding to the time coordinate, where it is $1$. Also, $\frac{\partial G_i}{\partial e_j}\equiv \frac{\partial F_i}{\partial e_j}$ and the first $n$ coordinates of  $\frac{\partial G}{\partial t}$ form a vector normal to $T_x X^n$ by (\ref{evolutioneq}).  Thus, the tangent space of $\mathrm{Im}(G)$ is $T_x X^n \times \{0\}\oplus  \mathrm{span}\langle (\nu, 1)\rangle$ and we can apply corollary  \ref{localgraphcorollary} to $\mathrm{Im}(G)$ to get a function $\tilde{f}$ such that 
	\begin{align*}
		(x_0, t)= \left[(p, 0) +  (s\nu, s)\right] + \tilde{f}(p, s)(\nu, -\Vert \nu\Vert)
	\end{align*}
	as $(\nu, -\Vert \nu\Vert)$ is the vector orthogonal to  $T_x X^n \times \{0\}\oplus  \mathrm{span}\langle (\nu, 1)\rangle$. Let $\sigma_p:I\subset\R\rightarrow\R$ be the function that associates to $t$ the appropriate $s$ in the expression above.  Projecting to the first $n+1$ coordinates and calling $f(p, t)=\sigma_p(t)+\tilde{f}(p, \sigma_p(t))$ one gets:
	\begin{align*}
		x_0= p + f(p, t)\nu 
	\end{align*}
	which is our thesis, as long as $\sigma_p(t)$ is smooth. This is indeed the case, as the graph function $\Gamma_f:x\mapsto(x, f(x))$ is smooth for any smooth function $f$ and has a smooth inverse (and thus, the inverse $(x_0, t)\mapsto ((p, 0) +  (s\nu, s))$ is smooth).
\end{proof}
One can show through direct calculation (see \cite{mantegazza}, Exercise 1.1.2) that, if an immersed hypersurface $\phi : M\rightarrow \R^{n+1}$ is locally the graph of a function (i.e., locally, $(x, f(x))=\phi$), then: 
\begin{align*}
	g_{ij}&=\delta_{ij}+ \frac{\partial f}{\partial x_i} \frac{\partial f}{\partial x_j}\\
	\nu&= -\frac{(\nabla f, -1)}{\sqrt{1+|\nabla f|^2}}\\
	h_{ij}&= -\frac{H_{ij}}{\sqrt{1+|\nabla f|^2}}
\end{align*}
Where the matrix $(H_{ij})_{ij}$ is the hessian of $f$. Thus, if one considers the principal curvatures, they are closely related to the eigenvalues of the hessian of $f$. 




\section{The Alexandrov Reflection Method and the Chow result}

%The Alexandrov Reflection method is a technique based on the Maximum Principle. 

The key idea of the main result in this chapter is as follows: suppose we have an embedded smooth hypersurface $M^n$ evolving according to (\ref{evolutioneq}) and a fixed hyperplane $\Pi$, intersecting $M^n$. We can also consider $M^n_\Pi$, the reflection of $M^n$ with respect to $\Pi$, which will also evolve according to (\ref{evolutioneq}). Suppose that, at some time $t$, $M^n$ and  $M^n_\Pi$ touch outside of $\Pi$. We can consider $M^n$ and  $M^n_\Pi$ as local graphs over the same hyperplane $\Pi$, and we can show that these function evolve according to the same differential equation. Using the strong maximum principle and the Hopf boundary point lemma, then, one can conclude that the two functions coincide, and have been coinciding up until that point. We can then conclude that if  $M^n$ and  $M^n_\Pi$ only touch on $M^n\cap\Pi$ at the beginning of the evolution, then they will never touch elsewhere. We shall make this more precise in the following paragraph, loosely based on paragraph 2 in \cite{Chow}. 

\section{Theorem 2.2 Chow [RENAME]}



Let $\Pi$ be a hyperplane in $\R^{n+1}$. We may assume $\Pi$ orthogonal to a unit vector $V\in\R^{n+1}$, i.e. $\langle\Pi, V\rangle= C$ for some constant $C$. 

Then, $R^{n+1}$ is divided by $\Pi$ into two half-spaces, which we will name  $H^+(\Pi)=\{x \in \R : \langle x, V\rangle > C\}$ and  $H^-(\Pi)=\{x \in \R | \langle x, V\rangle < C\}$. 

\begin{defin}
	We say we can reflect $M^n$ strictly with respect to $\Pi$ if 
	\begin{align*}
		M^n_\Pi\cap H^-(\Pi)\subset \mathrm{int}(M^n)\cap H^-(\Pi)
	\end{align*} 
	and $V\notin TM^n_x$ for all $x\in M^n \cap\Pi$, where $\mathrm{int}(M^n)$ is the region inside $M^n$. 
\end{defin}
This fundamentally means that the reflection of one of the halves of $M^n$ on the other side of $\Pi$ is contained in the region inside $M^n$ and the tangent spaces of $M^n$ and of the half-reflection do not form a ninety degree angle with $\Pi$, at all points on $\Pi\cap M^n$. 
\begin{defin}
	We say we can reflect $M^n$ strictly up to $(\Pi,V)$ if we can reflect $M^n$ strictly with respect to $\Pi_K$ for all hyperplanes $\Pi_K=\{x \in \R | \langle x, V\rangle = K\}$ such that $K<C$.  
\end{defin}

The main theorem is the following: 

\begin{theorem}[Chow]
	Let $X:M^n\times [0,T) \rightarrow \R^{n+1}$ be a $C^2$ solution to \ref{evolutioneq}. Then, if we can reflect $X(M^n, 0)=M_0$ strictly with respect to $\Pi$, then for all $t\in [0,T)$ we can reflect $X(M^n, t)=M_t$ strictly with respect to $\Pi$. \label{reflection} 
\end{theorem}

\begin{proof}
	By contradiction, suppose that there is a time $t$ such that the thesis is false, and that it is the smallest such $t$. Then, for all $\tau \in [0,t)$, $M_{\tau,\Pi}\cap H^-(\Pi)\subset \mathrm{int}(M_{\tau})\cap H^-(\Pi)$; the unit vector orthogonal to $\Pi$, $V$, is such that $V\notin T_xM_\tau$ for all $x\in M_\tau\cap \Pi$ and $\tau \in [0,t)$; and either of the conditions fails at $t$, i.e. either: 
	\begin{itemize}
		\item[(i)] $M_{t,\Pi}\cap H^-(\Pi)\cap M_{t}\neq \emptyset$
		\item[(ii)] $V\in T_xM_t$  for some $x\in\Pi$. 
	\end{itemize} 
	
	(i) Suppose the first case is true. Then, there exists $x_0 \in M_{t,\Pi}\cap H^-(\Pi)\cap M_{t}$ such that at $x_0$ the two manifolds are tangent. \\
	We can take a neighbourhood of $(x_0, t)\in M_{t}\times \R$ such that  both $M_{t,\Pi}$ and $M_{t}$ are graphs over $T_{x_0}M_{t}$ by \ref{localgraph}. \\
	We can explicitly write the functions $f:U\times (t-\varepsilon, t+\varepsilon)\rightarrow M_t$, where $U\subset T_{x_0}M_{t}$, and the corresponding $f_\Pi$ for $M_{t,\Pi}$. 
	We can also write 
	\begin{align*}
		f \; : \; (x, t) &\mapsto x+\tilde{f}(x, t)\nu \\
		f_\Pi \; : \; (x, t) &\mapsto x+\tilde{f}_\Pi(x, t)\nu 
	\end{align*}
	for appropriate functions $\tilde{f}:U\times (t-\varepsilon, t+\varepsilon)\rightarrow \R$ and $\tilde{f}_\Pi:U\times (t-\varepsilon, t+\varepsilon)\rightarrow \R$, where $\nu$ is a fixed unit vector normal to $T_{x_0}M_{t}$.  $\tilde{f}$ and $\tilde{f_\Pi}$ are solutions to the same second order PDE, which is parabolic by what was discussed in paragraph \ref{parabolic}, hence we can apply Proposition \ref{firstapplication} to conclude that $\tilde{f}\equiv\tilde{f_\Pi}$, and thus $M_{t,\Pi}$ and $M_{t}$ coincide in a neighbourhood of $(x, t)$, a contradiction as we assumed that $t$ is the first $t$ where the flows touch.
	
	(ii) Suppose instead that $V\in T_xM_t$  for some $t\in [0, t)$ and some $x\in M_t\cap \Pi$. Then $T_xM_t= T_xM_{t, \Pi}$ and in a neighbourhood of $(x, t)$ both $M_t$ and $M_{t, \Pi}$ are graphs of two smooth functions over $T_xM_t$ by \ref{localgraph}, i.e. again
	\begin{align*}
		f \; : \; (x, t) &\mapsto x+\tilde{f}(x, t)\nu \\
		f_\Pi \; : \; (x, t) &\mapsto x+\tilde{f}_\Pi(x, t)\nu 
	\end{align*} 
	Moreover, in $\overline{H^-(\Pi)}$, $f_\Pi\geq f$, because $M^n_\Pi\cap H^-(\Pi)\subset \mathrm{int}(M^n)\cap H^-(\Pi)$. Finally, $f(x, t)=f_\Pi (x, t)$, hence $f_\Pi-f (x, t)=0$, and thus  $(x, t)$ is a minimum point on the boundary for $f_\Pi-f$. Also, we must have
	\begin{align*}
		\frac{\partial f}{\partial V}(x,t)=\frac{\partial f_\Pi}{\partial V}(x,t)
	\end{align*}
	because the graphs are both tangent to $T_xM_t$, and $V$ here is the outward pointing normal to the boundary by definition of the reflection. Thus, 
	\begin{align*}
		\frac{\partial (f- f_\Pi)}{\partial V}(x,t)=0
	\end{align*}
	But we must have 
	\begin{align*}
		\frac{\partial (f- f_\Pi)}{\partial V}(x,t)>0
	\end{align*}
	at a minimum on the boundary by Proposition \ref{secondapplication}, a contradiction.  
\end{proof}
The following is an immediate consequence of the main result:
\begin{cor}
	Let $X:M^n\times [0,T) \rightarrow \R^{n+1}$ be a $C^2$ solution to \ref{evolutioneq}. Then, if we can reflect $M_0$ strictly up to $(\Pi, V)$, then for all $t\in [0,T)$ we can reflect $X(M, t)$ strictly up to $(\Pi, V)$.  
\end{cor}
\begin{proof}
	The hypothesis of the theorem are true for each $\Pi_K$ in the definition, thus we can reflect strictly with respect to each $\Pi_K$ for all $t\in[0,T)$, and thus we can reflect $X(M, t)$ strictly up to $(\Pi, V)$.
\end{proof}
